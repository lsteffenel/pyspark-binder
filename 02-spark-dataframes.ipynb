{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Y7gAwWDn7NYx"
      },
      "source": [
        "# Spark Structured API: DataFrames and SQL\n",
        "In the previous notebook you have seen distributed processing using RDDs is done. In this notebook we will look at Spark's Structured API. We will see how you can use DataFrames and SQL to do common data processing operations. By the end you should have a feeling for the strengths and weaknesses of these different approaches.\n",
        "\n",
        "The first difference is our Spark _entrypoint_. For RDDs this was the 'SparkContext' (usually named `sc`). For DataFrames we will use a 'SparkSession', which is more powerful and easier to use. By convention we name our SparkSession `spark`, and we create it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "ZB_sNKbxRLv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "tags": [],
        "id": "j0XBD_2L7NY0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "tSxllAF47NY1"
      },
      "source": [
        "We can use a SparkSession to create DataFrames (as we will soon see) and these can be converted to RDDs. However if we directly want to create RDDs we have to do this via SparkContext. A SparkContext is contained in SparkSession, and can be used as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "YhfBYXHV7NY1"
      },
      "outputs": [],
      "source": [
        "sc = spark.sparkContext\n",
        "rdd = sc.parallelize(['a', 'b', 'c'])\n",
        "rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hBs9DFPW7NY1"
      },
      "source": [
        "## DataFrames from Python collections\n",
        "\n",
        "Just like we have seen with `sc.parallelize` for RDDs, we can create a DataFrame from an existing Python collection. In addition to the collection itself we will also describe (part of) the structure of the data by naming the columns. Additionally, we could  specify the data types of the columns, but in this case we can let Spark infer this automatically.\n",
        "\n",
        "First, a list of tuples in Python is created, called `phone_stock`. Next, we create a list called `columns` that contain the name of all columns of the DataFrame. Then we use these two lists as input for [`createDataFrame`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame). The result is the DataFrame `phone_df`. Next we print the type of both `phone_stock` and `phone_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "R2dTEWXJ7NY1"
      },
      "outputs": [],
      "source": [
        "phone_stock = [\n",
        "    ('iPhone 6', 'Apple', 6, 549.00),\n",
        "    ('iPhone 6s', 'Apple', 5, 585.00),\n",
        "    ('iPhone 7', 'Apple', 11, 739.00),\n",
        "    ('Pixel', 'Google', 8, 859.00),\n",
        "    ('Pixel XL', 'Google', 2, 959.00),\n",
        "    ('Galaxy S7', 'Samsung', 10, 539.00),\n",
        "    ('Galaxy S6', 'Samsung', 5, 414.00),\n",
        "    ('Galaxy A5', 'Samsung', 7, 297.00),\n",
        "    ('Galaxy Note 7', 'Samsung', 0, 841.00)\n",
        "]\n",
        "\n",
        "columns = ['model', 'brand', 'stock', 'unit_price']\n",
        "\n",
        "phone_df = spark.createDataFrame(phone_stock, columns)\n",
        "\n",
        "print('the type of phoneStock: ' + str(type(phone_stock)))\n",
        "print('the type of phone_df: ' + str(type(phone_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "PxtJ1dJH7NY2"
      },
      "source": [
        "In order to see a few rows of a DataFrame use [`show()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show). By default it shows 20 rows, but you can give the desired number of rows that you want to see as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "OE_4n-Gq7NY2"
      },
      "outputs": [],
      "source": [
        "phone_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "gzJgsFcw7NY3"
      },
      "source": [
        "Like RDDs we have a [`collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) action that returns all data from a DataFrame to the driver. Notice that we get `Row` objects that contain column name and value pairs. Remember that the result of a `collect()` is a Python data structure (a list of `Row` objects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "_gqsqoF37NY3"
      },
      "outputs": [],
      "source": [
        "all_phones = phone_df.collect()\n",
        "all_phones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Reh2dllF7NY3"
      },
      "source": [
        "Working directly with a list of row objects is cumbersome. To work directly with data on the driver's side, we usually convert the Spark DataFrame to a `pandas` DataFrame. [`pandas`](https://pandas.pydata.org/) is a data processing library that allows us to manipulate tabular table. It is suitable for processing that isn't too intensive and data that isn't too large to fit into local memory (otherwise, why would we want to use Spark?).\n",
        "\n",
        "Spark DataFrames have a [`toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) action defined on them, that will pull all data to the driver and convert it to a `pandas` DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "i1PdA4aJ7NY3"
      },
      "outputs": [],
      "source": [
        "phone_df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "CGEENisw7NY3"
      },
      "source": [
        "There are several ways to look at the structure of a DataFrame: `printSchema`, `schema` and `describe`. [`printSchema`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema) is especially useful with complicated nested structures, because it provides a human-readable form:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "HRj1HkZ87NY3"
      },
      "outputs": [],
      "source": [
        "phone_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5eOnEaQ27NY4"
      },
      "source": [
        "Note that all columns are listed, together with their type and a boolean value that indicates whether the value for that column can be NULL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vCvDroJZ7NY4"
      },
      "source": [
        "Schema's can also be listed programmatically. By calling [`schema`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.schema) we get to see the structure of the DataFrame in Sparks types. It is possible to define a schema in code by making use of these types, although we won't do this here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "HvjTRNtP7NY4"
      },
      "outputs": [],
      "source": [
        "phone_df.schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "SDdthC9d7NY4"
      },
      "source": [
        "It is also possible to look more closely on the structure of fields, in which the columns are defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "vwTDuCWj7NY4"
      },
      "outputs": [],
      "source": [
        "phone_df.schema.fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "tikZXxKA7NY4"
      },
      "source": [
        "[`describe`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) will compute summary statistics for numeric and string columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JhVGGTsc7NY4"
      },
      "outputs": [],
      "source": [
        "phone_df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1sDodC327NY4"
      },
      "source": [
        "## Data extraction\n",
        "\n",
        "Now that we have our data in a DataFrame, we want to use it to manipulate the data. Let's start by selecting subsets of the data: specific columns and/or rows.\n",
        "\n",
        "### Selecting columns\n",
        "\n",
        "Often we are not interested in all the columns of our data. DataFrames make it very easy to select only a subset by using the [`select`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) method. Realise that we are not modifying the original DataFrame, but creating a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "49ML9OKJ7NY4"
      },
      "outputs": [],
      "source": [
        "# Select only the model column\n",
        "model_df = phone_df.select(\"model\")\n",
        "model_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "B-uo5tuG7NY5"
      },
      "source": [
        "We can also rename a column by using [`expr`](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.expr)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "hI_gY-eP7NY5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "mymodel_df = phone_df.select(\"brand\", expr(\"model as mymodel\"))\n",
        "mymodel_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-OPjy6YL7NY5"
      },
      "outputs": [],
      "source": [
        "# Select both the brand and model columns\n",
        "bm_df = phone_df.select('brand', 'model')\n",
        "bm_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NDfw55X87NY5"
      },
      "source": [
        "## Assignment 1\n",
        "Select the `model` and `stock` columns from `phone_df`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "fa8sYHxp7NY5"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Select the model and stock columns\n",
        "ms_df = phone_df.<FILL_IN>\n",
        "ms_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6GJFmptR7NY5"
      },
      "source": [
        "### Filtering rows\n",
        "\n",
        "We can filter specific rows by using the DataFrame [`filter`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter) method. Please note that the [`where`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where) method is an alias for `filter`. The column specifications are the same as with the select method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "scrolled": true,
        "id": "p9Oxpp097NY5"
      },
      "outputs": [],
      "source": [
        "# Select rows with phones from Google\n",
        "google_df = phone_df.filter(phone_df['brand'] == 'Google')\n",
        "\n",
        "google_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BmYCzX_b7NY5"
      },
      "source": [
        "## Assignment 2\n",
        "Select the rows with `unit_price` less than 550.00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "k0JUA-1y7NY5"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "\n",
        "cheap_df = phone_df.filter(<FILL IN>)\n",
        "cheap_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UM4YSl3a7NY5"
      },
      "source": [
        "Multiple filter conditions can be specified using Python's [boolean operations](https://docs.python.org/3/library/stdtypes.html#boolean-operations-and-or-not):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "CjCxAhOi7NY5"
      },
      "outputs": [],
      "source": [
        "phone_df.filter((phone_df.brand == 'Apple') | (phone_df.brand == 'Google')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GxDUtbu_7NY6"
      },
      "source": [
        "### Ordering rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xMv8c0ZB7NY6"
      },
      "source": [
        "We can use the [`orderBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) method to sort data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lZDf_gHB7NY6"
      },
      "outputs": [],
      "source": [
        "phone_df.orderBy('unit_price').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NBPt0W807NY6"
      },
      "source": [
        "#### Note: Columns specifications\n",
        "\n",
        "In the previous examples we have used various _column specifications_ for selecting and filtering data. Sometimes the more complicated ones are required because the shorter versions are ambiguous for Spark's parser. For example, all these are equivalent:\n",
        "\n",
        "```\n",
        "bm_df = phone_df.select(\"brand\", \"model\")\n",
        "bm_df = phone_df.select([\"brand\", \"model\"])\n",
        "bm_df = phone_df.select(phone_df[\"brand\"], phone_df[\"model\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "88OsBj-f7NY6"
      },
      "source": [
        "In the next cell we use a chain of DataFrame methods that are very similar to the SQL query language used for certain databases.\n",
        "    Notice that we use only the names of columns. Note, the use of double and single quotes in the [`where`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UZ2th8kV7NY9"
      },
      "outputs": [],
      "source": [
        "phone_df.select(\"model\", \"unit_price\").where(\"brand='Apple'\").orderBy('stock', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Yw1qxA5p7NY9"
      },
      "source": [
        "An alternative way of doing the same as the cell above is using `phone_df[\"brand\"]` in the where clause. This is longer to type but intuitively more clear and easier to read. There is no ambiguity for the Spark parser with this notation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cKbbtnMV7NY9"
      },
      "outputs": [],
      "source": [
        "phone_df.select(\"model\", \"unit_price\").where(phone_df[\"brand\"]==\"Apple\").orderBy('stock', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rx59FcD_7NY-"
      },
      "source": [
        "## Assignment 3\n",
        "Select all phones with a unit price larger than 300 and of which there are more than two in stock. Display the remaining phones, ordered by brand, followed by stock. Use whatever column specification syntax you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "UdZ1K6X37NY-"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6lMPRser7NY-"
      },
      "source": [
        "## Aggregating data\n",
        "An important part of data processing is the ability to combine multiple records, like we did with `reduceByKey`. In the DataFrame API this is a two-step process:\n",
        "\n",
        "First you group the data using the `groupBy` method. `groupBy` can operate on one or multiple columns. It will not actually perform the grouping but create a reference to a `GroupedData` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "etM-jqUw7NY-"
      },
      "outputs": [],
      "source": [
        "grouped_df = phone_df.groupBy('brand')\n",
        "print(type(grouped_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hykJeqte7NY-"
      },
      "source": [
        "After the data is grouped we can apply one of the standard aggregation functions on it. They are listed at the [GroupedData](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) API documentation. These are: `min`, `max`, `mean`, `sum` and `count`. We can apply an aggregation to all columns or to a subset of the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "uoeFh0qe7NY-"
      },
      "outputs": [],
      "source": [
        "# Minimum for all columns\n",
        "min_df = grouped_df.min('unit_price')\n",
        "\n",
        "min_df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cHOpvYpY7NY-"
      },
      "source": [
        "Notice that the `min(unit_price)` is the name of the new column. If you want to rename a column use [`withColumnRenamed`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed). As arguments this method takes the old name and new name of the column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Fm5C9C8y7NY-"
      },
      "source": [
        "## Assignment 4\n",
        "\n",
        "Compute the maximum  of the unit_price per brand and rename the resulting column to `max`.\n",
        "(We assume you can do this in one line. Feel free to adapt the cell and use more lines if you want.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4J7_SEC27NY-"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "max_df = <FILL IN>\n",
        "max_df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "w277Tz0K7NY_"
      },
      "source": [
        "Finally, we can combine different aggregations per column using the [`agg`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) method on a GroupedData instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "DM1m3NJ-7NY_"
      },
      "outputs": [],
      "source": [
        "# Take the sum of the stock column, and calculate the mean of the unit_price column, in one go\n",
        "sum_df = grouped_df.agg({'stock': 'sum', 'unit_price': 'mean'})\n",
        "\n",
        "sum_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Shjx8mIc7NY_"
      },
      "source": [
        "## SQL\n",
        "The SQL API aims to be ANSI-SQL SQL2003 and Hive-SQL compatible. The expressiveness is very similar to the DataFrame API. You can access the SQL API from the SparkSession by using `spark.sql`. Below is a query performed using Spark's DataFrame API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GpNt7O3G7NY_"
      },
      "outputs": [],
      "source": [
        "# DataFrame version\n",
        "res_df = phone_df.filter(phone_df['stock'] > 7).select('model')\n",
        "res_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_yrKxPIN7NY_"
      },
      "source": [
        "The SQL version of the query requires us to 'register' the DataFrame as an SQL table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ADW6uwsy7NY_"
      },
      "outputs": [],
      "source": [
        "# SQL version\n",
        "\n",
        "# Register the phone_df DataFrame within SQL as a table with name 'phones'\n",
        "phone_df.createOrReplaceTempView('phones')\n",
        "\n",
        "# Perform the SQL query on the 'phones' table\n",
        "res_df = spark.sql('SELECT model FROM phones WHERE stock > 7')\n",
        "res_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xhfgseai7NY_"
      },
      "source": [
        "## Joining with other data sets\n",
        "Often you want to combine multiple datasets on a shared column. In this example we create an extra table with information about the phone manufacturer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Kqy8060f7NY_"
      },
      "outputs": [],
      "source": [
        "companies = [\n",
        "    ('Google', 'USA', 1998, 'Sundar Pichai'),\n",
        "    ('Samsung', 'South Korea', 1938 ,'Oh-Hyun Kwon' ),\n",
        "    ('Apple', 'USA', 1976 ,'Tim Cook')\n",
        "]\n",
        "\n",
        "columns = ['company_name', 'hq_country', 'founding_year', 'ceo']\n",
        "\n",
        "company_df = spark.createDataFrame(companies, columns)\n",
        "company_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EwfT1dD07NZA"
      },
      "source": [
        "To join two DataFrames, we use the `join` method on one of the DataFrames. This method takes two arguments: (1) the other DataFrame, and (2) a join relation. Here we join the two DataFrames on the brand/company_name columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1TDOiMvv7NZA"
      },
      "outputs": [],
      "source": [
        "joined_df = phone_df.join(company_df, phone_df['brand'] == company_df['company_name'])\n",
        "joined_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZPtESzsZ7NZA"
      },
      "source": [
        "Here is an example of a more complicated query that combines multiple steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "xCCNb75Y7NZA"
      },
      "outputs": [],
      "source": [
        "# All the models from USA companies with more than 7 items in stock\n",
        "result = phone_df \\\n",
        "    .join(company_df, phone_df['brand'] == company_df['company_name']) \\\n",
        "    .filter(company_df['hq_country'] == 'USA') \\\n",
        "    .filter(phone_df['stock'] > 7) \\\n",
        "    .select('model')\n",
        "\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kDc_uA6E7NZB"
      },
      "source": [
        "## Assignment 5\n",
        "\n",
        "The problem below was taken from Coursera's MOOC [Big Data Analysis with Scala and Spark](https://www.coursera.org/learn/scala-spark-big-data) by the École Polytechnique Fédérale de Lausanne. We adapted the problem for PySpark.\n",
        "\n",
        "Let's assume we have a dataset with posts from a discussion forum. The entries of the dataset consist of an authorID, the name of a subforum, the number of likes and a date. The data frame is constructed in the following cell.\n",
        "\n",
        "**We would like to know how many likes each author posted on each subforum. The table should show per subforum how many likes each author has, the highest number of likes first.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GFFUYBvZ7NZB"
      },
      "outputs": [],
      "source": [
        "from  pyspark.sql import Row\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "\n",
        "posts = [{'authorID' : 4, 'subforum': 'java', 'likes': 5, 'date' : 'sept 5'},\n",
        "         {'authorID' : 1, 'subforum': 'python', 'likes': 3, 'date' : 'sept 4'},\n",
        "        {'authorID' : 2, 'subforum': 'python', 'likes': 35, 'date' : 'sept 3'},\n",
        "        {'authorID' : 3, 'subforum': 'java', 'likes': 1, 'date' : 'sept 5'},\n",
        "        {'authorID' : 4, 'subforum': 'java', 'likes': 14, 'date' : 'sept 5'},\n",
        "        {'authorID' : 3, 'subforum': 'python', 'likes': 12, 'date' : 'sept 3'},\n",
        "        {'authorID' : 3, 'subforum': 'java', 'likes': 14, 'date' : 'sept 5'},\n",
        "        {'authorID' : 3, 'subforum': 'java', 'likes': 10, 'date' : 'sept 5'},\n",
        "        {'authorID' : 2, 'subforum': 'python', 'likes': 21, 'date' : 'sept 5'}]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(posts)\n",
        "df_posts = spark.createDataFrame(rdd.map(lambda x : Row(**x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EDJYl0RN7NZB"
      },
      "source": [
        "Please use a [groupBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy), the [sum aggregation function](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum) and an [orderBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) to come up with the desired dataFrame. Note that you want to order in descending order.\n",
        "Also note, that you can use [`groupBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) and `orderBy` on more than one column.\n",
        "\n",
        "If you get confused, break the problem into steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b_RdWH1v7NZB"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "VDRy4or27NZB"
      },
      "source": [
        "## Conversion to/from RDD\n",
        "\n",
        "Sometimes you want to do data manipulations which would be very easy with RDD operations, but complicated with the DataFrame API. Fortunately you can convert between DataFrames and RDDs of type 'Row'. Going from DataFrame to RDD is quite simple. Going back from RDD to DataFrame is more difficult because you need to re-apply the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "LjiQp4pt7NZB"
      },
      "outputs": [],
      "source": [
        "phone_rdd = phone_df.rdd\n",
        "plural_rdd = phone_rdd.map(lambda r: r.brand + 's')\n",
        "plural_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ENWsh5vh7NZB"
      },
      "source": [
        "# Reading structured files/sources\n",
        "One of the advantages of DataFrames is the ability to read already structured data and automatically import the structure in Spark. Spark contains readers for a number of formats such as csv, json, parquet, orc, text and jdbc. There are also third-party readers/connectors for databases such as MongoDB and Cassandra.\n",
        "\n",
        "Here we read the json-formatted tweets that we also used in the last notebook. As you can see the complicated JSON schema is inferred."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lsteffenel/pyspark-binder/raw/master/tweets.json"
      ],
      "metadata": {
        "id": "T6xagZnR7Zew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b21zGGur7NZB"
      },
      "outputs": [],
      "source": [
        "tweet_df = spark.read.format(\"json\").load('tweets.json')\n",
        "tweet_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1RyhiIn07NZC"
      },
      "source": [
        "This structure is squeezed into a table. When we convert to Pandas we can see what the first tweet looks like in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "60HOv47A7NZC"
      },
      "outputs": [],
      "source": [
        "tweet_df.toPandas().head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3AwQPl3o7NZC"
      },
      "source": [
        "## Assignment 6\n",
        "Select the name and screen_name of the user, the text field and the lang field.\n",
        "\n",
        "**Hint**: nested fields can be selected using the dot notation, i.e. `df.select('<parent>.<child>')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Y3Ih9XMA7NZC"
      },
      "outputs": [],
      "source": [
        "name_df = tweet_df.<FILL IN>\n",
        "name_df.toPandas().head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "AC2Hcizh7NZC"
      },
      "source": [
        "## Assignment 7\n",
        "Count the number of tweets per user, and display the top 10 most-tweeting users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lYy6YID07NZC"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ySHfiyNt7NZC"
      },
      "source": [
        "## Word count in DataFrames\n",
        "\n",
        "It is also possible to use DataFrames for less-structured data such as text. Here we show how you could do word count with DataFrames.\n",
        "\n",
        "The following chained query contains a number of methods you haven't seen before, and we'll go through it line by line."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lsteffenel/pyspark-binder/raw/master/shakespeare.txt"
      ],
      "metadata": {
        "id": "ICH9zZ5Y7gct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IOD9iZSC7NZC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode, split\n",
        "\n",
        "spark \\\n",
        "    .read.text('shakespeare.txt') \\\n",
        "    .select(explode(split(\"value\", \"\\W+\")).alias(\"word\")) \\\n",
        "    .groupBy(\"word\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"count\", ascending=0).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8eCyKiYV7NZC"
      },
      "source": [
        "To see what happens here, we break it down into steps. First we read in the data file and inspect the DataFrame. It contains one column, called `value` by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Pq0nJyfO7NZC"
      },
      "outputs": [],
      "source": [
        "swan_df = spark.read.text('shakespeare.txt')\n",
        "swan_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2raJUmGl7NZC"
      },
      "source": [
        "The column name `value` explains why it is mentioned inside the `split` function. Let's call the `select` method but omit `explode` and see what happens. Notice, that with `alias` we rename the column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-nJg0efo7NZD"
      },
      "outputs": [],
      "source": [
        "split_df = swan_df.select(split(\"value\", \"\\W+\").alias(\"word\"))\n",
        "split_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GtEkJUbx7NZD"
      },
      "source": [
        "Looking at the schema, we can see that `word` is actually an array of strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "wrWYp9tX7NZD"
      },
      "outputs": [],
      "source": [
        "split_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7txqDpMd7NZD"
      },
      "source": [
        "Instead, we would like to have a row for each word, which is where [`explode`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) comes in. It has a similar meaning as `flatMap` in Spark RDDs. It gets rid of lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "O0mldJtn7NZD"
      },
      "outputs": [],
      "source": [
        "swan_df.select(explode(split(\"value\", \"\\W+\")).alias(\"word\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vWw16ldo7NZD"
      },
      "source": [
        "### User-defined functions\n",
        "\n",
        "In the previous example we used the built-in split function. It is also possible to define and use a custom user-defined function, or UDF. We'll show an example for the phone stock DataFrame first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "00qiTIbn7NZD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "exp_udf = udf(lambda price: \"Expensive\" if price >= 500 else \"Inexpensive\", StringType())\n",
        "\n",
        "phone_df.withColumn(\"cost\", exp_udf(phone_df['unit_price'])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jnrZSQ-M7NZD"
      },
      "source": [
        "In this manner, we can apply specialized function, like tokenizers, on DataFrames. However, we first must register them as UDFs and cannot simply define them inline with lambda functions like we can with RDDs.\n",
        "\n",
        "Below we define a very simple tokenizer, just as an example. It uses Python's string `split`, and also lowers the case of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kUERqQvq7NZD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "def my_tokenize(s):\n",
        "    s = s.lower()\n",
        "    words = s.split()\n",
        "    return words\n",
        "\n",
        "returnType = ArrayType(StringType())\n",
        "\n",
        "tokenize_udf = udf(my_tokenize, returnType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "dolmvHbU7NZD"
      },
      "source": [
        "## Assignment 8\n",
        "Use the `my_tokenize` function from the last cell to count words on the Shakespeare DataFrame `swan_df` instead of usng the `split` function. Display the top 10 most occurring words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ZbZPXIFg7NZE"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QJbkhTgO7NZE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}