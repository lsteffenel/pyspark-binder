{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Spark - Utilisation des DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Présentation \n",
    "\n",
    "Le projet open data de la ville de San Francisco (The SF OpenData project) a été lancé en 2009 et contient des centaines de datasets concernant la ville et l'agglomération de San Francisco.\n",
    "\n",
    "Dans ce TP nous allons analyser les appels aux pompiers (SF Fire Department), à la recherche de réponses pour les questions suivantes :\n",
    "\n",
    "* Combien de types différents d'appel ont été enregistrés ?\n",
    "* Combien d'incidents de chaque type ont été recensés ?\n",
    "* Combien d'années sont enregistrées dans le fichier ?\n",
    "* Combien d'appels ont été enregistrés lors d'une semaine donnée ? \n",
    "* Quel quartier a généré plus d'appels en 2004 ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset\n",
    "!wget http://cosy.univ-reims.fr/~lsteffenel/cours/Master2/RT0902-BigData/Fire_Department_Calls_for_Service.csv -O dataset/Fire_Department_Calls_for_Service.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des données\n",
    "Pour commencer, nous allons lire une base de données déjà sur HDFS.\n",
    "Cette base est en format CSV. Au lieu de parser nous mêmes le fichier, nous allons demander à Spark de faire la lecture, de parser le fichier et même de deviner le type de donnée des colonnes.\n",
    "Observez aussi l'option `header=True` qui indique à Spark de ne pas inclure la première ligne de la table (mais de l'utiliser pour le nom des colonnes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName('TP Datasets Spark').getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%time fireServiceCallsDF = spark.read.csv(\"/home/jovyan/dataset/Fire_Department_Calls_for_Service.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'opération précédente a pris du temps non à cause de la taille du fichier (presque 400MB) mais pour une raison plus bête : \n",
    "\n",
    "* Spark a dû deviner le schéma (le type de données) de chaque colonne. Pour cela, il a lu le fichier afin de vérifier si les types devinés ne sont pas en conflit (par exemple, supposer qu'une colonne est composée d'entiers mais vers la fin on retrouve des string)\n",
    "\n",
    "Une manière d'accélérer ce processus est de fournir le schéma (si on le connaît), ce qui a aussi l'avantage d'être plus précis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n",
    "fireSchema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "                     StructField('UnitID', StringType(), True),\n",
    "                     StructField('IncidentNumber', IntegerType(), True),\n",
    "                     StructField('CallType', StringType(), True),                  \n",
    "                     StructField('CallDate', StringType(), True),       \n",
    "                     StructField('WatchDate', StringType(), True),       \n",
    "                     StructField('ReceivedDtTm', StringType(), True),       \n",
    "                     StructField('EntryDtTm', StringType(), True),       \n",
    "                     StructField('DispatchDtTm', StringType(), True),       \n",
    "                     StructField('ResponseDtTm', StringType(), True),       \n",
    "                     StructField('OnSceneDtTm', StringType(), True),       \n",
    "                     StructField('TransportDtTm', StringType(), True),                  \n",
    "                     StructField('HospitalDtTm', StringType(), True),       \n",
    "                     StructField('CallFinalDisposition', StringType(), True),       \n",
    "                     StructField('AvailableDtTm', StringType(), True),       \n",
    "                     StructField('Address', StringType(), True),       \n",
    "                     StructField('City', StringType(), True),       \n",
    "                     StructField('ZipcodeofIncident', IntegerType(), True),       \n",
    "                     StructField('Battalion', StringType(), True),                 \n",
    "                     StructField('StationArea', StringType(), True),       \n",
    "                     StructField('Box', StringType(), True),       \n",
    "                     StructField('OriginalPriority', StringType(), True),       \n",
    "                     StructField('Priority', StringType(), True),       \n",
    "                     StructField('FinalPriority', IntegerType(), True),       \n",
    "                     StructField('ALSUnit', BooleanType(), True),       \n",
    "                     StructField('CallTypeGroup', StringType(), True),\n",
    "                     StructField('NumberofAlarms', IntegerType(), True),\n",
    "                     StructField('UnitType', StringType(), True),\n",
    "                     StructField('Unitsequenceincalldispatch', IntegerType(), True),\n",
    "                     StructField('FirePreventionDistrict', StringType(), True),\n",
    "                     StructField('SupervisorDistrict', StringType(), True),\n",
    "                     StructField('NeighborhoodDistrict', StringType(), True),\n",
    "                     StructField('Location', StringType(), True),\n",
    "                     StructField('RowID', StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Observez comment ça va plus vite ! Comme il ne faut plus deviner (en regardant le fichier), Spark accepte le schéma sans perdre du temps \n",
    "# Attention : comme Python est un langage non-typé, nous n'obtenons que des DataFrame (les Datasets, plus avancés encore, sont disponibles pour Scala)\n",
    "\n",
    "%time fireServiceCallsDF = spark.read.csv(\"/home/jovyan/dataset/Fire_Department_Calls_for_Service.csv\", header=True, schema=fireSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ici on a un extrait des 5 premières lignes du fichier.\n",
    "sample=fireServiceCallsDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#On peut vérifier le schéma avec la méthode \"printSchema()\"\n",
    "\n",
    "\n",
    "fireServiceCallsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# il est aussi possible d'imprimer juste le nom des colonnes\n",
    "\n",
    "fireServiceCallsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# combien de lignes il y a dans le DataFrame ? (ça devrait afficher 1190109 entrées)\n",
    "# Ça prend beaucoup de temps car il faut quand même parcourir tout le fichier.\n",
    "\n",
    "fireServiceCallsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouvrez la documentation de Spark 2.0 dans un autre onglet, afin de pouvoir consulter rapidement l'API :\n",
    "\n",
    "1. Spark 2.0 docs: http://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "2. DataFrame user documentation: http://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "3. PySpark API 2.0 docs: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les DataFrames supportent deux types d'operations : les transformations et les actions.\n",
    "\n",
    "![Transfos](http://cosy.univ-reims.fr/~lsteffenel/images/trans_and_actions.png \"Transformations et actions\")\n",
    "\n",
    "Les transformations comme select() ou filter() créent un nouveau DataFrame à partir d'un DataFrame existant.\n",
    "\n",
    "Les actions telles que show() ou count() effectuent une action qui résulte en un résultat retourné à l'utilisateur. Certaines actions comme save() sont utilisées pour écrire le DataFrame dans le système de stockage distribué.\n",
    "\n",
    "**Les transformations contribuent à la planification de la requête, mais rien d'est vraiment executé jusqu'à ce qu'une action soit appelée.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 : Combien de types différents d’appel ont été enregistrés ? Listez tous les différents types.\n",
    "\n",
    "Astuce : Pour répondre à cette question, utilisez les méthodes `select()` et `distinct()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# dans cet exemple on affiche juste les 5 premiers types\n",
    "fireServiceCallsDF.select('CallType').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# combient de types différents ? Affichez juste une fois chacun des types\n",
    "# votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : Combien d’incidents de chaque type ont été recensés ?\n",
    "\n",
    "Astuce : Utilisez `groupBy()` et `count()`, voir même `orderBy()` pour trier l'affichage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : Combien d’années sont enregistrées dans le fichier ?\n",
    "\n",
    "Si vous faites attention, les colonnes relatives aux dates et horaires sont interprétées comme des string et pas comme des dates ou timestamps.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "`fireServiceCallsDF.printSchema()`\n",
    "root\n",
    " |-- CallNumber: integer (nullable = true)\n",
    " |-- UnitID: string (nullable = true)\n",
    " |-- IncidentNumber: integer (nullable = true)\n",
    " |-- CallType: string (nullable = true)\n",
    " |-- CallDate: string (nullable = true)\n",
    " |-- WatchDate: string (nullable = true)\n",
    " |-- ReceivedDtTm: string (nullable = true)\n",
    " |-- EntryDtTm: string (nullable = true)\n",
    " |-- DispatchDtTm: string (nullable = true)\n",
    " |-- ResponseDtTm: string (nullable = true)\n",
    " |-- OnSceneDtTm: string (nullable = true)\n",
    " |-- TransportDtTm: string (nullable = true)\n",
    " |-- HospitalDtTm: string (nullable = true)\n",
    " |-- CallFinalDisposition: string (nullable = true)\n",
    " |-- AvailableDtTm: string (nullable = true)\n",
    " |-- Address: string (nullable = true)\n",
    " |-- City: string (nullable = true)\n",
    " |-- ZipcodeofIncident: integer (nullable = true)\n",
    " |-- Battalion: string (nullable = true)\n",
    " |-- StationArea: string (nullable = true)\n",
    " |-- Box: string (nullable = true)\n",
    " |-- OriginalPriority: string (nullable = true)\n",
    " |-- Priority: string (nullable = true)\n",
    " |-- FinalPriority: integer (nullable = true)\n",
    " |-- ALSUnit: boolean (nullable = true)\n",
    " |-- CallTypeGroup: string (nullable = true)\n",
    " |-- NumberofAlarms: integer (nullable = true)\n",
    " |-- UnitType: string (nullable = true)\n",
    " |-- Unitsequenceincalldispatch: integer (nullable = true)\n",
    " |-- FirePreventionDistrict: string (nullable = true)\n",
    " |-- SupervisorDistrict: string (nullable = true)\n",
    " |-- NeighborhoodDistrict: string (nullable = true)\n",
    " |-- Location: string (nullable = true)\n",
    " |-- RowID: string (nullable = true)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nous pouvons utiliser la fonction `unix_timestamp()` afin de convertir les string en timestamp:\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.from_unixtime](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.from_unixtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from_pattern1 = 'MM/dd/yyyy'\n",
    "to_pattern1 = 'yyyy-MM-dd'\n",
    "\n",
    "from_pattern2 = 'MM/dd/yyyy hh:mm:ss aa'\n",
    "to_pattern2 = 'MM/dd/yyyy hh:mm:ss aa'\n",
    "\n",
    "# Création d'un nouveau DataFrame qui utilise les dates converties en timestamp\n",
    "fireServiceCallsTsDF = fireServiceCallsDF \\\n",
    "  .withColumn('CallDateTS', unix_timestamp(fireServiceCallsDF['CallDate'], from_pattern1).cast(\"timestamp\")) \\\n",
    "  .drop('CallDate') \\\n",
    "  .withColumn('WatchDateTS', unix_timestamp(fireServiceCallsDF['WatchDate'], from_pattern1).cast(\"timestamp\")) \\\n",
    "  .drop('WatchDate') \\\n",
    "  .withColumn('ReceivedDtTmTS', unix_timestamp(fireServiceCallsDF['ReceivedDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('ReceivedDtTm') \\\n",
    "  .withColumn('EntryDtTmTS', unix_timestamp(fireServiceCallsDF['EntryDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('EntryDtTm') \\\n",
    "  .withColumn('DispatchDtTmTS', unix_timestamp(fireServiceCallsDF['DispatchDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('DispatchDtTm') \\\n",
    "  .withColumn('ResponseDtTmTS', unix_timestamp(fireServiceCallsDF['ResponseDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('ResponseDtTm') \\\n",
    "  .withColumn('OnSceneDtTmTS', unix_timestamp(fireServiceCallsDF['OnSceneDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('OnSceneDtTm') \\\n",
    "  .withColumn('TransportDtTmTS', unix_timestamp(fireServiceCallsDF['TransportDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('TransportDtTm') \\\n",
    "  .withColumn('HospitalDtTmTS', unix_timestamp(fireServiceCallsDF['HospitalDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('HospitalDtTm') \\\n",
    "  .withColumn('AvailableDtTmTS', unix_timestamp(fireServiceCallsDF['AvailableDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('AvailableDtTm')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "fireServiceCallsTsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Notez comme l'affichage des dates/timestamps est différente\n",
    "fireServiceCallsTsDF.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, trouvez la liste d'années qui sont enregistrées dans le CSV.\n",
    "Vous pouvez utiliser la fonction sql `year()` pour vous aider : \n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : Combien d’appels ont été enregistrés lors des 7 derniers jours ?\n",
    "\n",
    "ATTENTION : Considérez que nous sommes le 15 juin 2003 (le 166ème jour de l'année). Utilisez la date sur le colonne `CallDateTS`.\n",
    "\n",
    "Vous pouvez filtrer les entrées avec les deux fonctions utilitaires SQL `year()` et `dayofyear()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note à propos de la performance\n",
    "\n",
    "Si vous avez fait attention, toutes les opérations précédentes prendent un certain temps à s'exécuter. L'une des raisons est le fait que à chaque fois on oblige la lecture du fichier. Il serait plus rapide si on pouvait garder ces données en cache mémoire, pour un accès plus rapide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# ici on va définir une view qui sera mise en cache lors de l'exécution d'une action\n",
    "fireServiceCallsTsDF.createOrReplaceTempView(\"fireServiceVIEW\")\n",
    "spark.catalog.cacheTable(\"fireServiceVIEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# on fait l'action count() pour mettre la table en mémoire.  \n",
    "spark.table(\"fireServiceVIEW\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "fireServiceDF = spark.table(\"fireServiceVIEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Le count() fait sur un DF en mémoire cache est bien plus rapide !!!\n",
    "fireServiceDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jusqu'à présent nous avons fait des requêtes utilisant la syntaxe des Dataframes. \n",
    "Spark supporte aussi une syntaxe SQL à partir de la bibliothèse Spark.SQL. \n",
    "\n",
    "Si on programme directement en python, on pourrait faire des appels comme ceci : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"SELECT count(*) FROM fireServiceVIEW\")\n",
    "sqlWay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : Quel quartier a généré plus d'appels en 2002 ?\n",
    "\n",
    "Utilisez `spark.sql` pour faire votre requête. \n",
    "Le quartier est indiqué dans la colonne `NeighborhoodDistrict`. Pour les requêtes, utilisez le dataset `fireServiceVIEW` qui est déjà en cache. Limitez la réponse aux 15 quartiers avec le plus grand nombre d'appels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
