{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Spark - Utilisation de Spark Streaming\n",
    "\n",
    "## ETAPE 1 : Cloner ce notebook\n",
    "* Cliquer sur le bouton \"clone this note\", ci-dessus (5ème bouton à côté de \"TP-Spark-Streaming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP nous allons utiliser des datasets issus du contrôle aérien des États Unis. Dans ce cas, nous avons des données concernant différents vols au sein des États Unis en 2014. Ceux-ci listent les différents vols, leur lieux de départ et de destination, la distance parcourue, la date et aussi les retards. \n",
    "\n",
    "Un fichier en format json a déjà été enregistré sur HDFS, nous allons définir le schéma et charger ce fichier en mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "departureDelays_geo_schema = StructType([StructField(\"tripid\",IntegerType(),True),\n",
    "                    StructField(\"localdate\",TimestampType(),True),\n",
    "                    StructField(\"delay\",IntegerType(),True),\n",
    "                    StructField(\"distance\",IntegerType(),True),\n",
    "                    StructField(\"src\",StringType(),True),\n",
    "                    StructField(\"dst\",StringType(),True),\n",
    "                    StructField(\"city_src\",StringType(),True),\n",
    "                    StructField(\"city_dst\",StringType(),True),\n",
    "                    StructField(\"state_src\",StringType(),True),\n",
    "                    StructField(\"state_dst\",StringType(),True)])\n",
    "    \n",
    "    \n",
    "\n",
    "departureDelays_static = (spark.read.schema(departureDelays_geo_schema).json(\"/user/zeppelin/data-source/departureDelays_all.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `departureDelays_static`, comme son nom l'indique, est un DataFrame statique (type *batch*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(departureDelays_static)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous voulons obtenir le nombre de vols par état (`departureDelays_static.state_src`), regroupés par des \"fenêtres\" de temps de 5h.\n",
    "Pour cela, utilisez la fonction `window()` (un exemple est disponible à https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=groupby#pyspark.sql.functions.window ).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(staticCountsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Maintenant on va garder ce DataFrame en mémoire cache, et créer une table qu'on peut requêter avec du SQL\n",
    "staticCountsDF.cache()\n",
    "\n",
    "\n",
    "staticCountsDF.createOrReplaceTempView(\"static_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la prochaine ligne fait directement du SQL. Ici, on récupère les données précédentes, en faisant la transformation de la date (on extrait la date et l'heure finale de chaque période)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select state_src, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts order by time, state_src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'example précédent a utilise des DataFrames statiques. Dans la suite, nous allons faire l'usage du Structured Streaming de Spark.\n",
    "\n",
    "Pour simuler un stream, nous allons prendre un répertoire avec plusieurs fichiers. Ces fichiers sont en fait le même fichier source précédent, découpé en 200 fichiers différents de manière plus ou moins aléatoire.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "hdfs dfs -ls /user/zeppelin/data-source/departureDelays_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de lire ce stream, nous allons \"connecter\" le stream à une source fichier. Nous allons \"tricher\" un peu, en obligeant la lecture d'un fichier à la fois... Cela nous permettra (on espère) de voir l'évolution des tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "inputPath = \"/user/zeppelin/data-source/departureDelays_json\"\n",
    "\n",
    "# On utilise la même définition est schéma de staticInputDF, seulement on fera l'usage de `readStream` au lieu de `read`\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .schema(departureDelays_geo_schema) # Le schéma des fichiers JSON\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Traite les fichiers comme un stream, en lisant un à la fois\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Est-ce vraiment un stream ? La sortie de `isStreaming` indique quel est le status de ce DataFrame\n",
    "streamingInputDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# On prépare la même requête que précédemment. Attention, le stream n'a pas encore démarré, on a juste mis en place la source et une opération\n",
    "streamingCountsDF = (                 \n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.state_src, \n",
    "      window(streamingInputDF.localdate, \"5 hours\"))    \n",
    "    .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le moment est arrivé\n",
    "\n",
    "Et voilà, c'est le moment de démarrer le traitement du stream. Dans le paragraphe suivant vous allez déclancher un query (un tableau dynamique), qui sera alimentée par le stream (les fichiers).\n",
    "Pour observer cela, on a fait plusiers requêtes SQL intercalées par des sleep. Les résultats affichés dans les paragraphes suivants devraient être modifiées au fil du temps, indiquant l'annexation de nouvelles informations selon l'arrivage des données du stream.  \n",
    "\n",
    " *Aucune garantie sur le résultat :) Ça a marché pour moi mais tout dépend de la vitesse de traitement des fichiers (si tout est fait rapidement, même la première requête sera déjà complète)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")  # keep the size of shuffles small\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"count3\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "sql1 = spark.sql(\"select state_src, date_format(window.end, 'MMM-dd HH:mm') as time, count from count3 order by time,state_src\")\n",
    "sleep(5)  # wait a bit \n",
    "sql2 = spark.sql(\"select state_src, date_format(window.end, 'MMM-dd HH:mm') as time, count from count3 order by time,state_src\")\n",
    "sleep(5)  # wait a bit more\n",
    "sql3 = spark.sql(\"select state_src, date_format(window.end, 'MMM-dd HH:mm') as time, count from count3 order by time,state_src\")\n",
    "sleep(5)  # wait a bit more\n",
    "sql4 = spark.sql(\"select state_src, date_format(window.end, 'MMM-dd HH:mm') as time, count from count3 order by time,state_src\")\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(sql1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(sql2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(sql3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(sql4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
