{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/pyspark-binder/blob/master/02-spark-dataframes-fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Y7gAwWDn7NYx"
      },
      "source": [
        "# Spark SQL: DataFrames\n",
        "Dans ce notebook, nous allons nous intéresser à l'API SQL Spark. Nous verrons comment utiliser les DataFrames et SQL pour effectuer des opérations courantes de traitement de données.\n",
        "\n",
        "Pour commencer, nous allons créer un `SparkSession`, qui est une demande d'accès au framework Spark (un peu comme ouvrir un ticket pour un service de dépannage informatique)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": true,
        "editable": true,
        "tags": [],
        "id": "j0XBD_2L7NY0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hBs9DFPW7NY1"
      },
      "source": [
        "## DataFrames à partir de collections Python\n",
        "\n",
        "Dans le cours nous avons fait un petit exemple où nous avons crée un DataFrame à partir d'un fichier **csv**. Il est aussi possible de créer un DataFrame à partir d'une liste Python existante. En plus de la liste elle-même, nous allons également décrire (en partie) la structure des données en nommant les colonnes. En outre, nous pourrions spécifier les types de données des colonnes, mais dans ce cas, nous pouvons laisser Spark déduire cela automatiquement.\n",
        "\n",
        "Tout d'abord, une liste de tuples en Python est créée, appelée `phone_stock`. Ensuite, nous créons une liste appelée `columns` qui contient le nom de toutes les colonnes du DataFrame. Puis nous utilisons ces deux listes comme entrée pour [`createDataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.createDataFrame.html). Le résultat est le DataFrame `phone_df`. Ensuite, nous imprimons le type de `phone_stock` et de `phone_df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "R2dTEWXJ7NY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c53466-4978-42a7-8985-1c5a32d9299f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the type of phoneStock: <class 'list'>\n",
            "the type of phone_df: <class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "phone_stock = [\n",
        "    ('iPhone 6', 'Apple', 6, 549.00),\n",
        "    ('iPhone 6s', 'Apple', 5, 585.00),\n",
        "    ('iPhone 7', 'Apple', 11, 739.00),\n",
        "    ('Pixel', 'Google', 8, 859.00),\n",
        "    ('Pixel XL', 'Google', 2, 959.00),\n",
        "    ('Galaxy S7', 'Samsung', 10, 539.00),\n",
        "    ('Galaxy S6', 'Samsung', 5, 414.00),\n",
        "    ('Galaxy A5', 'Samsung', 7, 297.00),\n",
        "    ('Galaxy Note 7', 'Samsung', 0, 841.00)\n",
        "]\n",
        "\n",
        "columns = ['model', 'brand', 'stock', 'unit_price']\n",
        "\n",
        "phone_df = spark.createDataFrame(phone_stock, columns)\n",
        "\n",
        "print('the type of phoneStock: ' + str(type(phone_stock)))\n",
        "print('the type of phone_df: ' + str(type(phone_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "PxtJ1dJH7NY2"
      },
      "source": [
        "Pour afficher quelques lignes d'un DataFrame, utilisez [`show()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html). Par défaut, il affiche 20 lignes, mais vous pouvez donner le nombre de lignes que vous souhaitez voir comme argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "OE_4n-Gq7NY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487a002e-bf95-4053-979b-96b928eaea54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+-----+----------+\n",
            "|        model|  brand|stock|unit_price|\n",
            "+-------------+-------+-----+----------+\n",
            "|     iPhone 6|  Apple|    6|     549.0|\n",
            "|    iPhone 6s|  Apple|    5|     585.0|\n",
            "|     iPhone 7|  Apple|   11|     739.0|\n",
            "|        Pixel| Google|    8|     859.0|\n",
            "|     Pixel XL| Google|    2|     959.0|\n",
            "|    Galaxy S7|Samsung|   10|     539.0|\n",
            "|    Galaxy S6|Samsung|    5|     414.0|\n",
            "|    Galaxy A5|Samsung|    7|     297.0|\n",
            "|Galaxy Note 7|Samsung|    0|     841.0|\n",
            "+-------------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "gzJgsFcw7NY3"
      },
      "source": [
        "Dans les DataFrame Spark, l'exécution n'est pas immédiate (on appelle ça une exécution *lazy* ou paresseuse) : on attend l'enchaînement des opérations pour essayer d'optimiser l'exécution. Le traitement s'effectue seulement quand faisons une action comme [`collect()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html).\n",
        "\n",
        "Dans le cas ci-dessous, nous obtenons des objets `Row` qui contiennent des paires de noms de colonnes et de valeurs. En effet, le résultat d'une action `collect()` est une structure de données Python (une liste d'objets `Row` dans cet exemple)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "_gqsqoF37NY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17136c68-0a63-4cf4-f241-2130b94ebd7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(model='iPhone 6', brand='Apple', stock=6, unit_price=549.0),\n",
              " Row(model='iPhone 6s', brand='Apple', stock=5, unit_price=585.0),\n",
              " Row(model='iPhone 7', brand='Apple', stock=11, unit_price=739.0),\n",
              " Row(model='Pixel', brand='Google', stock=8, unit_price=859.0),\n",
              " Row(model='Pixel XL', brand='Google', stock=2, unit_price=959.0),\n",
              " Row(model='Galaxy S7', brand='Samsung', stock=10, unit_price=539.0),\n",
              " Row(model='Galaxy S6', brand='Samsung', stock=5, unit_price=414.0),\n",
              " Row(model='Galaxy A5', brand='Samsung', stock=7, unit_price=297.0),\n",
              " Row(model='Galaxy Note 7', brand='Samsung', stock=0, unit_price=841.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "all_phones = phone_df.collect()\n",
        "all_phones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "CGEENisw7NY3"
      },
      "source": [
        "Il y a plusieurs façons d'examiner la structure d'un DataFrame : `printSchema`, `schema` et `describe`. [`printSchema`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.printSchema.html) est particulièrement utile avec les structures imbriquées compliquées, parce qu'il fournit une forme lisible :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "HRj1HkZ87NY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8522bbad-0b45-42be-d5f8-922b007ab322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- model: string (nullable = true)\n",
            " |-- brand: string (nullable = true)\n",
            " |-- stock: long (nullable = true)\n",
            " |-- unit_price: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5eOnEaQ27NY4"
      },
      "source": [
        "Notez que toutes les colonnes sont répertoriées, avec leur type et une valeur booléenne qui indique si la valeur de cette colonne peut être NULL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "tikZXxKA7NY4"
      },
      "source": [
        "[`describe`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html) calcule des statistiques sommaires pour les colonnes numériques et les chaînes de caractères :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JhVGGTsc7NY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7942634a-7519-4227-ccb5-a69a8a71a90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-------+------------------+------------------+\n",
            "|summary|    model|  brand|             stock|        unit_price|\n",
            "+-------+---------+-------+------------------+------------------+\n",
            "|  count|        9|      9|                 9|                 9|\n",
            "|   mean|     NULL|   NULL|               6.0| 642.4444444444445|\n",
            "| stddev|     NULL|   NULL|3.5355339059327378|220.82295573100586|\n",
            "|    min|Galaxy A5|  Apple|                 0|             297.0|\n",
            "|    max| iPhone 7|Samsung|                11|             959.0|\n",
            "+-------+---------+-------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1sDodC327NY4"
      },
      "source": [
        "## Extraction de données\n",
        "\n",
        "Maintenant que nous avons nos données dans un DataFrame, nous voulons l'utiliser pour manipuler les données. Commençons par sélectionner des sous-ensembles de données : des colonnes et/ou des lignes spécifiques.\n",
        "\n",
        "### Sélection de colonnes\n",
        "\n",
        "Souvent, toutes les colonnes de nos données ne nous intéressent pas. Les DataFrames permettent de sélectionner très facilement un sous-ensemble en utilisant la méthode [`select`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html). Il faut savoir que nous ne modifions pas le DataFrame original, mais que nous en créons un nouveau."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "49ML9OKJ7NY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b5f7fb-e68f-4212-991a-e16186709ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|        model|\n",
            "+-------------+\n",
            "|     iPhone 6|\n",
            "|    iPhone 6s|\n",
            "|     iPhone 7|\n",
            "|        Pixel|\n",
            "|     Pixel XL|\n",
            "|    Galaxy S7|\n",
            "|    Galaxy S6|\n",
            "|    Galaxy A5|\n",
            "|Galaxy Note 7|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select only the model column\n",
        "model_df = phone_df.select(\"model\")\n",
        "model_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-OPjy6YL7NY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd673dd-a693-416d-9173-4bcdd9c24a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+\n",
            "|  brand|        model|\n",
            "+-------+-------------+\n",
            "|  Apple|     iPhone 6|\n",
            "|  Apple|    iPhone 6s|\n",
            "|  Apple|     iPhone 7|\n",
            "| Google|        Pixel|\n",
            "| Google|     Pixel XL|\n",
            "|Samsung|    Galaxy S7|\n",
            "|Samsung|    Galaxy S6|\n",
            "|Samsung|    Galaxy A5|\n",
            "|Samsung|Galaxy Note 7|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select both the brand and model columns\n",
        "bm_df = phone_df.select('brand', 'model')\n",
        "bm_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NDfw55X87NY5"
      },
      "source": [
        "## Exercice 1\n",
        "Sélectionner les colonnes `model` et `stock` de `phone_df`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "fa8sYHxp7NY5"
      },
      "outputs": [],
      "source": [
        "# TODO: Remplacer <FILL IN> avec le code approprié\n",
        "ms_df = phone_df.<FILL_IN>\n",
        "ms_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6GJFmptR7NY5"
      },
      "source": [
        "### Filtrage des lignes\n",
        "\n",
        "Nous pouvons filtrer des lignes spécifiques en utilisant la méthode DataFrame [`filter`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html). Veuillez noter que la méthode [`where`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.where.html) est un alias de `filter`. Les spécifications des colonnes sont les mêmes que pour la méthode select :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": true,
        "editable": true,
        "scrolled": true,
        "id": "p9Oxpp097NY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c50409c-da1c-4b80-fe8f-d35e9dcb598c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----+----------+\n",
            "|   model| brand|stock|unit_price|\n",
            "+--------+------+-----+----------+\n",
            "|   Pixel|Google|    8|     859.0|\n",
            "|Pixel XL|Google|    2|     959.0|\n",
            "+--------+------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sélectionner des lignes avec des téléphones Google\n",
        "google_df = phone_df.filter(phone_df['brand'] == 'Google')\n",
        "\n",
        "google_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BmYCzX_b7NY5"
      },
      "source": [
        "## Exercice 2\n",
        "Sélectionner les lignes avec `unit_price` inférieur à 550.00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "k0JUA-1y7NY5"
      },
      "outputs": [],
      "source": [
        "# TODO: Remplacer <FILL IN> avec le code approprié\n",
        "\n",
        "cheap_df = phone_df.filter(<FILL IN>)\n",
        "cheap_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UM4YSl3a7NY5"
      },
      "source": [
        "Des conditions de filtrage multiples peuvent être spécifiées à l'aide des [opérations booléennes](https://docs.python.org/3/library/stdtypes.html#boolean-operations-and-or-not) de Python :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "CjCxAhOi7NY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8a31dc-5958-4b41-f4fa-7fc270a69590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+-----+----------+\n",
            "|    model| brand|stock|unit_price|\n",
            "+---------+------+-----+----------+\n",
            "| iPhone 6| Apple|    6|     549.0|\n",
            "|iPhone 6s| Apple|    5|     585.0|\n",
            "| iPhone 7| Apple|   11|     739.0|\n",
            "|    Pixel|Google|    8|     859.0|\n",
            "| Pixel XL|Google|    2|     959.0|\n",
            "+---------+------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.filter((phone_df.brand == 'Apple') | (phone_df.brand == 'Google')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GxDUtbu_7NY6"
      },
      "source": [
        "### Trier les lignes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xMv8c0ZB7NY6"
      },
      "source": [
        "Nous pouvons utiliser l'opération [`orderBy`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html) pour trier les données :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lZDf_gHB7NY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a320edbf-631d-48b5-d398-f41328755101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+-----+----------+\n",
            "|        model|  brand|stock|unit_price|\n",
            "+-------------+-------+-----+----------+\n",
            "|    Galaxy A5|Samsung|    7|     297.0|\n",
            "|    Galaxy S6|Samsung|    5|     414.0|\n",
            "|    Galaxy S7|Samsung|   10|     539.0|\n",
            "|     iPhone 6|  Apple|    6|     549.0|\n",
            "|    iPhone 6s|  Apple|    5|     585.0|\n",
            "|     iPhone 7|  Apple|   11|     739.0|\n",
            "|Galaxy Note 7|Samsung|    0|     841.0|\n",
            "|        Pixel| Google|    8|     859.0|\n",
            "|     Pixel XL| Google|    2|     959.0|\n",
            "+-------------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.orderBy('unit_price').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "88OsBj-f7NY6"
      },
      "source": [
        "Dans la cellule suivante, nous utilisons une chaîne de méthodes DataFrame qui sont très similaires au langage de requête SQL utilisé pour certaines bases de données.\n",
        "    Notez que nous n'utilisons que les noms des colonnes. Notez également l'utilisation de guillemets doubles et simples dans la méthode [`where`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.where.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UZ2th8kV7NY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66857142-a71c-498e-cc70-d629d41dd914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|    model|unit_price|\n",
            "+---------+----------+\n",
            "| iPhone 7|     739.0|\n",
            "| iPhone 6|     549.0|\n",
            "|iPhone 6s|     585.0|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.select(\"model\", \"unit_price\").where(\"brand='Apple'\").orderBy('stock', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Yw1qxA5p7NY9"
      },
      "source": [
        "Une autre façon de faire la même chose que la cellule ci-dessus est d'utiliser `phone_df[« brand »]` dans la clause where. C'est plus long à taper mais intuitivement plus clair et plus facile à lire. Il n'y a pas d'ambiguïté pour l'analyseur Spark avec cette notation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cKbbtnMV7NY9"
      },
      "outputs": [],
      "source": [
        "phone_df.select(\"model\", \"unit_price\").where(phone_df[\"brand\"]==\"Apple\").orderBy('stock', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rx59FcD_7NY-"
      },
      "source": [
        "## Exercice 3\n",
        "Sélectionner tous les téléphones dont le prix unitaire est supérieur à 300 et dont il y a plus de deux en stock. Affichez les téléphones restants, classés par marque, puis par stock. Utilisez la syntaxe de spécification de colonne que vous préférez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "UdZ1K6X37NY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "df756a06-6e67-4f39-efe2-e2c07096b804"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-13-45a4860a2a25>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-45a4860a2a25>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    <FILL IN>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# TODO: Remplacer <FILL IN> avec le code approprié\n",
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6lMPRser7NY-"
      },
      "source": [
        " ## Agrégation des données\n",
        "Une partie importante du traitement des données est la possibilité de combiner plusieurs enregistrements. Dans l'API DataFrame, il s'agit d'un processus en deux étapes :\n",
        "\n",
        "D'abord, vous regroupez les données en utilisant la méthode `groupBy`. `groupBy` peut opérer sur une ou plusieurs colonnes. Elle n'effectue pas le regroupement mais crée une référence à un objet `GroupedData` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "etM-jqUw7NY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272706fa-bcd3-476c-ad5d-2a2f2881e50d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.group.GroupedData'>\n"
          ]
        }
      ],
      "source": [
        "grouped_df = phone_df.groupBy('brand')\n",
        "print(type(grouped_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hykJeqte7NY-"
      },
      "source": [
        "Une fois les données regroupées, nous pouvons leur appliquer l'une des fonctions d'agrégation standard. Elles sont répertoriées dans la documentation de l'API [GroupedData](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html) de l'API. Il s'agit de : `min`, `max`, `mean`, `sum` et `count`.\n",
        "\n",
        "Nous pouvons appliquer une agrégation à toutes les colonnes ou à un sous-ensemble de colonnes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "uoeFh0qe7NY-"
      },
      "outputs": [],
      "source": [
        "# Minimum for all columns\n",
        "min_df = grouped_df.min('unit_price')\n",
        "\n",
        "min_df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cHOpvYpY7NY-"
      },
      "source": [
        "Notez que `min(unit_price)` est le nom de la nouvelle colonne.\n",
        "\n",
        "Si vous souhaitez renommer une colonne, utilisez [`withColumnRenamed`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnRenamed.html). Cette méthode prend comme arguments l'ancien et le nouveau nom de la colonne.\n",
        "\n",
        "Notez également qu'on a transformé le DataFrame Spark en DataFrame Pandas avec `toPandas`. Si on voulait rester 'pure Spark', on pourrait faire appel à `show()`, par exemple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Fm5C9C8y7NY-"
      },
      "source": [
        "## Exercice 4\n",
        "\n",
        "Calculez le maximum du prix unitaire par marque et renommez la colonne résultante en `max`.\n",
        "\n",
        "Nous supposons que vous pouvez faire cela en une seule ligne. N'hésitez pas à adapter la cellule et à utiliser plus de lignes si vous le souhaitez.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4J7_SEC27NY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77765de-cddc-4e00-c049-02ee56557e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+\n",
            "|  brand|max(unit_price)|\n",
            "+-------+---------------+\n",
            "| Google|          959.0|\n",
            "|  Apple|          739.0|\n",
            "|Samsung|          841.0|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Remplacer <FILL IN> avec le code approprié\n",
        "max_df = <FILL IN>\n",
        "max_df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "w277Tz0K7NY_"
      },
      "source": [
        "Dans certains cas, on souhaite effectuer des aggrégations différentes sur plusieurs colonnes. Dans ce cas, nous pouvons combiner les différentes agrégations par colonne en utilisant la méthode [`agg`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) sur une instance de GroupedData :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "DM1m3NJ-7NY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c5eeb2-123a-4484-82a9-79dc48b9f52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------------+\n",
            "|  brand|sum(stock)|  avg(unit_price)|\n",
            "+-------+----------+-----------------+\n",
            "| Google|        10|            909.0|\n",
            "|  Apple|        22|624.3333333333334|\n",
            "|Samsung|        22|           522.75|\n",
            "+-------+----------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fait la somme de la colonne stock et la moyenne de la colonne unit_price\n",
        "sum_df = grouped_df.agg({'stock': 'sum', 'unit_price': 'mean'})\n",
        "\n",
        "sum_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Shjx8mIc7NY_"
      },
      "source": [
        "## SQL\n",
        "L'API SQL permet d'interroger les dataframes avec une syntaxe compatible SQL (utilisée dans la plupart des bases de données). Vous pouvez accéder à l'API SQL à partir de la session Spark en utilisant `spark.sql`. Voici comparaison entre une requête effectuée en utilisant l'API DataFrame de Spark et une requête avec l'API SQL :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GpNt7O3G7NY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cbf7a5-b0cf-46d2-83c2-9116c874ecc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|    model|\n",
            "+---------+\n",
            "| iPhone 7|\n",
            "|    Pixel|\n",
            "|Galaxy S7|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DataFrame version\n",
        "res_df = phone_df.filter(phone_df['stock'] > 7).select('model')\n",
        "res_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_yrKxPIN7NY_"
      },
      "source": [
        "La version SQL de la requête exige que nous « enregistrions » le DataFrame en tant que table SQL :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ADW6uwsy7NY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34b13ca-3cc4-44b3-95ad-e04706e38544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|    model|\n",
            "+---------+\n",
            "| iPhone 7|\n",
            "|    Pixel|\n",
            "|Galaxy S7|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SQL version\n",
        "\n",
        "# Register the phone_df DataFrame within SQL as a table with name 'phones'\n",
        "phone_df.createOrReplaceTempView('phones')\n",
        "\n",
        "# Perform the SQL query on the 'phones' table\n",
        "res_df = spark.sql('SELECT model FROM phones WHERE stock > 7')\n",
        "res_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xhfgseai7NY_"
      },
      "source": [
        "## Jointure avec d'autres ensembles de données\n",
        "Il arrive souvent que vous souhaitiez combiner plusieurs ensembles de données en se basant sur une même colonne (clé commune). Dans cet exemple avec l'API Spark DataFrame, nous créons une table supplémentaire contenant des informations sur le fabricant du téléphone :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Kqy8060f7NY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d2cf32-0a33-4bb1-a508-48c346caa579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+-------------+-------------+\n",
            "|company_name| hq_country|founding_year|          ceo|\n",
            "+------------+-----------+-------------+-------------+\n",
            "|      Google|        USA|         1998|Sundar Pichai|\n",
            "|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|       Apple|        USA|         1976|     Tim Cook|\n",
            "+------------+-----------+-------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "companies = [\n",
        "    ('Google', 'USA', 1998, 'Sundar Pichai'),\n",
        "    ('Samsung', 'South Korea', 1938 ,'Oh-Hyun Kwon' ),\n",
        "    ('Apple', 'USA', 1976 ,'Tim Cook')\n",
        "]\n",
        "\n",
        "columns = ['company_name', 'hq_country', 'founding_year', 'ceo']\n",
        "\n",
        "company_df = spark.createDataFrame(companies, columns)\n",
        "company_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EwfT1dD07NZA"
      },
      "source": [
        "Pour joindre deux DataFrames, nous utilisons la méthode `join` sur l'un des DataFrames. Cette méthode prend deux arguments :  \n",
        "\n",
        "1.   l'autre DataFrame, et\n",
        "2.   une relation de jointure.\n",
        "\n",
        "Ici, nous joignons les deux DataFrames sur les colonnes brand/company_name :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1TDOiMvv7NZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5411c3-7965-41ac-d6e4-b8656147f5ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "|        model|  brand|stock|unit_price|company_name| hq_country|founding_year|          ceo|\n",
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "|     iPhone 6|  Apple|    6|     549.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|    iPhone 6s|  Apple|    5|     585.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|     iPhone 7|  Apple|   11|     739.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|        Pixel| Google|    8|     859.0|      Google|        USA|         1998|Sundar Pichai|\n",
            "|     Pixel XL| Google|    2|     959.0|      Google|        USA|         1998|Sundar Pichai|\n",
            "|    Galaxy S7|Samsung|   10|     539.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|    Galaxy S6|Samsung|    5|     414.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|    Galaxy A5|Samsung|    7|     297.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|Galaxy Note 7|Samsung|    0|     841.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joined_df = phone_df.join(company_df, phone_df['brand'] == company_df['company_name'])\n",
        "joined_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, nous avons la même jointure en utilisant SQL. Notez qu'on doit également déclarer le dataframe `company_df` en tant que table (avec le nom `company`)."
      ],
      "metadata": {
        "id": "TTS8ZH5ZGKJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the phone_df DataFrame within SQL as a table with name 'phones'\n",
        "company_df.createOrReplaceTempView('company')\n",
        "\n",
        "# Perform the SQL query on the 'phones' table\n",
        "join_df = spark.sql('SELECT * FROM phones JOIN company ON phones.brand = company.company_name')\n",
        "join_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbj_IdJQFaQk",
        "outputId": "ffdfa189-2f65-4c1d-8bbe-78205f174b4a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "|        model|  brand|stock|unit_price|company_name| hq_country|founding_year|          ceo|\n",
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "|     iPhone 6|  Apple|    6|     549.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|    iPhone 6s|  Apple|    5|     585.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|     iPhone 7|  Apple|   11|     739.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|        Pixel| Google|    8|     859.0|      Google|        USA|         1998|Sundar Pichai|\n",
            "|     Pixel XL| Google|    2|     959.0|      Google|        USA|         1998|Sundar Pichai|\n",
            "|    Galaxy S7|Samsung|   10|     539.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|    Galaxy S6|Samsung|    5|     414.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|    Galaxy A5|Samsung|    7|     297.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|Galaxy Note 7|Samsung|    0|     841.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZPtESzsZ7NZA"
      },
      "source": [
        "Here is an example of a more complicated query that combines multiple steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "xCCNb75Y7NZA"
      },
      "outputs": [],
      "source": [
        "# All the models from USA companies with more than 7 items in stock\n",
        "result = phone_df \\\n",
        "    .join(company_df, phone_df['brand'] == company_df['company_name']) \\\n",
        "    .filter(company_df['hq_country'] == 'USA') \\\n",
        "    .filter(phone_df['stock'] > 7) \\\n",
        "    .select('model')\n",
        "\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ENWsh5vh7NZB"
      },
      "source": [
        "# Lecture de fichiers/sources structurés\n",
        "L'un des avantages des DataFrames est la possibilité de lire des données déjà structurées et d'importer automatiquement la structure dans Spark. Spark contient des lecteurs pour un certain nombre de formats tels que csv, json, parquet, orc, text et jdbc. Il existe également des lecteurs/connecteurs tiers pour des bases de données telles que MongoDB et Cassandra.\n",
        "\n",
        "Dans cet exemple, nous allons lire des tweets au format json. Comme vous pouvez le voir, le schéma du fichier JSON est déduit automatiquement."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lsteffenel/pyspark-binder/raw/master/tweets.json"
      ],
      "metadata": {
        "id": "T6xagZnR7Zew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9742b39c-c398-44d6-88b4-9bd4b3c88034"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-02 18:37:00--  https://github.com/lsteffenel/pyspark-binder/raw/master/tweets.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/lsteffenel/pyspark-binder/master/tweets.json [following]\n",
            "--2025-03-02 18:37:01--  https://raw.githubusercontent.com/lsteffenel/pyspark-binder/master/tweets.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1629848 (1.6M) [text/plain]\n",
            "Saving to: ‘tweets.json’\n",
            "\n",
            "tweets.json         100%[===================>]   1.55M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-03-02 18:37:01 (21.9 MB/s) - ‘tweets.json’ saved [1629848/1629848]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b21zGGur7NZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30bd196-e193-49ee-ba09-11dc5b6d4642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- country: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- place: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tweet_df = spark.read.format(\"json\").load('tweets.json')\n",
        "tweet_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1RyhiIn07NZC"
      },
      "source": [
        "Cette structure est comprimée dans un tableau, nous pouvons voir à quoi ressemble le premier tweet dans un DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "60HOv47A7NZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b8bfd4-09e5-4fd3-b2a3-e9be9fedcbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------+--------------------+---------------+\n",
            "|country|                id| place|                text|           user|\n",
            "+-------+------------------+------+--------------------+---------------+\n",
            "|  India|572692378957430785|Orissa|@always_nidhi @Yo...|Srkian_nishu :)|\n",
            "+-------+------------------+------+--------------------+---------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tweet_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df.toPandas().head(1)"
      ],
      "metadata": {
        "id": "p7Zrg8scKyqK",
        "outputId": "13d362f0-b598-4e81-b6d4-51a0e40d60cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  country                  id   place  \\\n",
              "0   India  572692378957430785  Orissa   \n",
              "\n",
              "                                                text             user  \n",
              "0  @always_nidhi @YouTube no i dnt understand bt ...  Srkian_nishu :)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c684d9fa-ef2c-4637-a48e-efafb2b14451\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>id</th>\n",
              "      <th>place</th>\n",
              "      <th>text</th>\n",
              "      <th>user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>India</td>\n",
              "      <td>572692378957430785</td>\n",
              "      <td>Orissa</td>\n",
              "      <td>@always_nidhi @YouTube no i dnt understand bt ...</td>\n",
              "      <td>Srkian_nishu :)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c684d9fa-ef2c-4637-a48e-efafb2b14451')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c684d9fa-ef2c-4637-a48e-efafb2b14451 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c684d9fa-ef2c-4637-a48e-efafb2b14451');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"tweet_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"India\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"572692378957430785\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"place\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Orissa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"@always_nidhi @YouTube no i dnt understand bt i loved the music nd their dance awesome all the song of this mve is rocking\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Srkian_nishu :)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3AwQPl3o7NZC"
      },
      "source": [
        "## Exercice 5\n",
        "Sélectionnez le nom et le nom d'écran de l'utilisateur, le champ texte et le champ lang.\n",
        "\n",
        "**Astuce** : les champs imbriqués peuvent être sélectionnés en utilisant la notation par points, c'est-à-dire `df.select('<parent>.<child>')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Y3Ih9XMA7NZC"
      },
      "outputs": [],
      "source": [
        "name_df = tweet_df.<FILL IN>\n",
        "name_df.toPandas().head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "AC2Hcizh7NZC"
      },
      "source": [
        "## Assignment 7\n",
        "Count the number of tweets per user, and display the top 10 most-tweeting users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lYy6YID07NZC"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ySHfiyNt7NZC"
      },
      "source": [
        "## Word count in DataFrames\n",
        "\n",
        "It is also possible to use DataFrames for less-structured data such as text. Here we show how you could do word count with DataFrames.\n",
        "\n",
        "The following chained query contains a number of methods you haven't seen before, and we'll go through it line by line."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lsteffenel/pyspark-binder/raw/master/shakespeare.txt"
      ],
      "metadata": {
        "id": "ICH9zZ5Y7gct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IOD9iZSC7NZC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode, split\n",
        "\n",
        "spark \\\n",
        "    .read.text('shakespeare.txt') \\\n",
        "    .select(explode(split(\"value\", \"\\W+\")).alias(\"word\")) \\\n",
        "    .groupBy(\"word\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"count\", ascending=0).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8eCyKiYV7NZC"
      },
      "source": [
        "To see what happens here, we break it down into steps. First we read in the data file and inspect the DataFrame. It contains one column, called `value` by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Pq0nJyfO7NZC"
      },
      "outputs": [],
      "source": [
        "swan_df = spark.read.text('shakespeare.txt')\n",
        "swan_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2raJUmGl7NZC"
      },
      "source": [
        "The column name `value` explains why it is mentioned inside the `split` function. Let's call the `select` method but omit `explode` and see what happens. Notice, that with `alias` we rename the column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-nJg0efo7NZD"
      },
      "outputs": [],
      "source": [
        "split_df = swan_df.select(split(\"value\", \"\\W+\").alias(\"word\"))\n",
        "split_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GtEkJUbx7NZD"
      },
      "source": [
        "Looking at the schema, we can see that `word` is actually an array of strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "wrWYp9tX7NZD"
      },
      "outputs": [],
      "source": [
        "split_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7txqDpMd7NZD"
      },
      "source": [
        "Instead, we would like to have a row for each word, which is where [`explode`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) comes in. It has a similar meaning as `flatMap` in Spark RDDs. It gets rid of lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "O0mldJtn7NZD"
      },
      "outputs": [],
      "source": [
        "swan_df.select(explode(split(\"value\", \"\\W+\")).alias(\"word\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vWw16ldo7NZD"
      },
      "source": [
        "### User-defined functions\n",
        "\n",
        "In the previous example we used the built-in split function. It is also possible to define and use a custom user-defined function, or UDF. We'll show an example for the phone stock DataFrame first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "00qiTIbn7NZD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "exp_udf = udf(lambda price: \"Expensive\" if price >= 500 else \"Inexpensive\", StringType())\n",
        "\n",
        "phone_df.withColumn(\"cost\", exp_udf(phone_df['unit_price'])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jnrZSQ-M7NZD"
      },
      "source": [
        "In this manner, we can apply specialized function, like tokenizers, on DataFrames. However, we first must register them as UDFs and cannot simply define them inline with lambda functions like we can with RDDs.\n",
        "\n",
        "Below we define a very simple tokenizer, just as an example. It uses Python's string `split`, and also lowers the case of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kUERqQvq7NZD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "def my_tokenize(s):\n",
        "    s = s.lower()\n",
        "    words = s.split()\n",
        "    return words\n",
        "\n",
        "returnType = ArrayType(StringType())\n",
        "\n",
        "tokenize_udf = udf(my_tokenize, returnType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "dolmvHbU7NZD"
      },
      "source": [
        "## Assignment 8\n",
        "Use the `my_tokenize` function from the last cell to count words on the Shakespeare DataFrame `swan_df` instead of usng the `split` function. Display the top 10 most occurring words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ZbZPXIFg7NZE"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QJbkhTgO7NZE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}