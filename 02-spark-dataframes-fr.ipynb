{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/pyspark-binder/blob/master/02-spark-dataframes-fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Y7gAwWDn7NYx"
      },
      "source": [
        "# Spark SQL: DataFrames\n",
        "Dans ce notebook, nous allons nous intéresser à l'API SQL Spark. Nous verrons comment utiliser les DataFrames et SQL pour effectuer des opérations courantes de traitement de données.\n",
        "\n",
        "Pour commencer, nous allons créer un `SparkSession`, qui est une demande d'accès au framework Spark (un peu comme ouvrir un ticket pour un service de dépannage informatique)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": true,
        "editable": true,
        "tags": [],
        "id": "j0XBD_2L7NY0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hBs9DFPW7NY1"
      },
      "source": [
        "## DataFrames à partir de collections Python\n",
        "\n",
        "Dans le cours nous avons fait un petit exemple où nous avons crée un DataFrame à partir d'un fichier **csv**. Il est aussi possible de créer un DataFrame à partir d'une liste Python existante. En plus de la liste elle-même, nous allons également décrire (en partie) la structure des données en nommant les colonnes. En outre, nous pourrions spécifier les types de données des colonnes, mais dans ce cas, nous pouvons laisser Spark déduire cela automatiquement.\n",
        "\n",
        "Tout d'abord, une liste de tuples en Python est créée, appelée `phone_stock`. Ensuite, nous créons une liste appelée `columns` qui contient le nom de toutes les colonnes du DataFrame. Puis nous utilisons ces deux listes comme entrée pour [`createDataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.createDataFrame.html). Le résultat est le DataFrame `phone_df`. Ensuite, nous imprimons le type de `phone_stock` et de `phone_df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "R2dTEWXJ7NY1",
        "outputId": "c3c53466-4978-42a7-8985-1c5a32d9299f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the type of phoneStock: <class 'list'>\n",
            "the type of phone_df: <class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "phone_stock = [\n",
        "    ('iPhone 6', 'Apple', 6, 549.00),\n",
        "    ('iPhone 6s', 'Apple', 5, 585.00),\n",
        "    ('iPhone 7', 'Apple', 11, 739.00),\n",
        "    ('Pixel', 'Google', 8, 859.00),\n",
        "    ('Pixel XL', 'Google', 2, 959.00),\n",
        "    ('Galaxy S7', 'Samsung', 10, 539.00),\n",
        "    ('Galaxy S6', 'Samsung', 5, 414.00),\n",
        "    ('Galaxy A5', 'Samsung', 7, 297.00),\n",
        "    ('Galaxy Note 7', 'Samsung', 0, 841.00)\n",
        "]\n",
        "\n",
        "columns = ['model', 'brand', 'stock', 'unit_price']\n",
        "\n",
        "phone_df = spark.createDataFrame(phone_stock, columns)\n",
        "\n",
        "print('the type of phoneStock: ' + str(type(phone_stock)))\n",
        "print('the type of phone_df: ' + str(type(phone_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "PxtJ1dJH7NY2"
      },
      "source": [
        "Pour afficher quelques lignes d'un DataFrame, utilisez [`show()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html). Par défaut, il affiche 20 lignes, mais vous pouvez donner le nombre de lignes que vous souhaitez voir comme argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "OE_4n-Gq7NY2",
        "outputId": "487a002e-bf95-4053-979b-96b928eaea54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+-----+----------+\n",
            "|        model|  brand|stock|unit_price|\n",
            "+-------------+-------+-----+----------+\n",
            "|     iPhone 6|  Apple|    6|     549.0|\n",
            "|    iPhone 6s|  Apple|    5|     585.0|\n",
            "|     iPhone 7|  Apple|   11|     739.0|\n",
            "|        Pixel| Google|    8|     859.0|\n",
            "|     Pixel XL| Google|    2|     959.0|\n",
            "|    Galaxy S7|Samsung|   10|     539.0|\n",
            "|    Galaxy S6|Samsung|    5|     414.0|\n",
            "|    Galaxy A5|Samsung|    7|     297.0|\n",
            "|Galaxy Note 7|Samsung|    0|     841.0|\n",
            "+-------------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "gzJgsFcw7NY3"
      },
      "source": [
        "Dans les DataFrame Spark, l'exécution n'est pas immédiate (on appelle ça une exécution *lazy* ou paresseuse) : on attend l'enchaînement des opérations pour essayer d'optimiser l'exécution. Le traitement s'effectue seulement quand faisons une action comme [`collect()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html).\n",
        "\n",
        "Dans le cas ci-dessous, nous obtenons des objets `Row` qui contiennent des paires de noms de colonnes et de valeurs. En effet, le résultat d'une action `collect()` est une structure de données Python (une liste d'objets `Row` dans cet exemple)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "_gqsqoF37NY3",
        "outputId": "17136c68-0a63-4cf4-f241-2130b94ebd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(model='iPhone 6', brand='Apple', stock=6, unit_price=549.0),\n",
              " Row(model='iPhone 6s', brand='Apple', stock=5, unit_price=585.0),\n",
              " Row(model='iPhone 7', brand='Apple', stock=11, unit_price=739.0),\n",
              " Row(model='Pixel', brand='Google', stock=8, unit_price=859.0),\n",
              " Row(model='Pixel XL', brand='Google', stock=2, unit_price=959.0),\n",
              " Row(model='Galaxy S7', brand='Samsung', stock=10, unit_price=539.0),\n",
              " Row(model='Galaxy S6', brand='Samsung', stock=5, unit_price=414.0),\n",
              " Row(model='Galaxy A5', brand='Samsung', stock=7, unit_price=297.0),\n",
              " Row(model='Galaxy Note 7', brand='Samsung', stock=0, unit_price=841.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "all_phones = phone_df.collect()\n",
        "all_phones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "CGEENisw7NY3"
      },
      "source": [
        "Il y a plusieurs façons d'examiner la structure d'un DataFrame : `printSchema`, `schema` et `describe`. [`printSchema`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.printSchema.html) est particulièrement utile avec les structures imbriquées compliquées, parce qu'il fournit une forme lisible :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "HRj1HkZ87NY3",
        "outputId": "8522bbad-0b45-42be-d5f8-922b007ab322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- model: string (nullable = true)\n",
            " |-- brand: string (nullable = true)\n",
            " |-- stock: long (nullable = true)\n",
            " |-- unit_price: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5eOnEaQ27NY4"
      },
      "source": [
        "Notez que toutes les colonnes sont répertoriées, avec leur type et une valeur booléenne qui indique si la valeur de cette colonne peut être NULL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "tikZXxKA7NY4"
      },
      "source": [
        "[`describe`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html) calcule des statistiques sommaires pour les colonnes numériques et les chaînes de caractères :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JhVGGTsc7NY4",
        "outputId": "7942634a-7519-4227-ccb5-a69a8a71a90a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-------+------------------+------------------+\n",
            "|summary|    model|  brand|             stock|        unit_price|\n",
            "+-------+---------+-------+------------------+------------------+\n",
            "|  count|        9|      9|                 9|                 9|\n",
            "|   mean|     NULL|   NULL|               6.0| 642.4444444444445|\n",
            "| stddev|     NULL|   NULL|3.5355339059327378|220.82295573100586|\n",
            "|    min|Galaxy A5|  Apple|                 0|             297.0|\n",
            "|    max| iPhone 7|Samsung|                11|             959.0|\n",
            "+-------+---------+-------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1sDodC327NY4"
      },
      "source": [
        "## Extraction de données\n",
        "\n",
        "Maintenant que nous avons nos données dans un DataFrame, nous voulons l'utiliser pour manipuler les données. Commençons par sélectionner des sous-ensembles de données : des colonnes et/ou des lignes spécifiques.\n",
        "\n",
        "### Sélection de colonnes\n",
        "\n",
        "Souvent, toutes les colonnes de nos données ne nous intéressent pas. Les DataFrames permettent de sélectionner très facilement un sous-ensemble en utilisant la méthode [`select`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html). Il faut savoir que nous ne modifions pas le DataFrame original, mais que nous en créons un nouveau."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "49ML9OKJ7NY4",
        "outputId": "b1b5f7fb-e68f-4212-991a-e16186709ed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|        model|\n",
            "+-------------+\n",
            "|     iPhone 6|\n",
            "|    iPhone 6s|\n",
            "|     iPhone 7|\n",
            "|        Pixel|\n",
            "|     Pixel XL|\n",
            "|    Galaxy S7|\n",
            "|    Galaxy S6|\n",
            "|    Galaxy A5|\n",
            "|Galaxy Note 7|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select only the model column\n",
        "model_df = phone_df.select(\"model\")\n",
        "model_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-OPjy6YL7NY5",
        "outputId": "fcd673dd-a693-416d-9173-4bcdd9c24a0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+\n",
            "|  brand|        model|\n",
            "+-------+-------------+\n",
            "|  Apple|     iPhone 6|\n",
            "|  Apple|    iPhone 6s|\n",
            "|  Apple|     iPhone 7|\n",
            "| Google|        Pixel|\n",
            "| Google|     Pixel XL|\n",
            "|Samsung|    Galaxy S7|\n",
            "|Samsung|    Galaxy S6|\n",
            "|Samsung|    Galaxy A5|\n",
            "|Samsung|Galaxy Note 7|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select both the brand and model columns\n",
        "bm_df = phone_df.select('brand', 'model')\n",
        "bm_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NDfw55X87NY5"
      },
      "source": [
        "## Exercice 1\n",
        "Sélectionner les colonnes `model` et `stock` de `phone_df`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "fa8sYHxp7NY5"
      },
      "outputs": [],
      "source": [
        "# TODO: Remplacer <FILL IN> avec le code approprié\n",
        "ms_df = phone_df.<FILL_IN>\n",
        "ms_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6GJFmptR7NY5"
      },
      "source": [
        "### Filtrage des lignes\n",
        "\n",
        "Nous pouvons filtrer des lignes spécifiques en utilisant la méthode DataFrame [`filter`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html). Veuillez noter que la méthode [`where`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.where.html) est un alias de `filter`. Les spécifications des colonnes sont les mêmes que pour la méthode select :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": true,
        "editable": true,
        "scrolled": true,
        "id": "p9Oxpp097NY5",
        "outputId": "5c50409c-da1c-4b80-fe8f-d35e9dcb598c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----+----------+\n",
            "|   model| brand|stock|unit_price|\n",
            "+--------+------+-----+----------+\n",
            "|   Pixel|Google|    8|     859.0|\n",
            "|Pixel XL|Google|    2|     959.0|\n",
            "+--------+------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sélectionner des lignes avec des téléphones Google\n",
        "google_df = phone_df.filter(phone_df['brand'] == 'Google')\n",
        "\n",
        "google_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BmYCzX_b7NY5"
      },
      "source": [
        "## Exercice 2\n",
        "Sélectionner les lignes avec `unit_price` inférieur à 550.00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "k0JUA-1y7NY5"
      },
      "outputs": [],
      "source": [
        "# TODO: Remplacer <FILL IN> avec le code approprié\n",
        "\n",
        "cheap_df = phone_df.filter(<FILL IN>)\n",
        "cheap_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UM4YSl3a7NY5"
      },
      "source": [
        "Des conditions de filtrage multiples peuvent être spécifiées à l'aide des [opérations booléennes](https://docs.python.org/3/library/stdtypes.html#boolean-operations-and-or-not) de Python :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "CjCxAhOi7NY5",
        "outputId": "fc8a31dc-5958-4b41-f4fa-7fc270a69590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+-----+----------+\n",
            "|    model| brand|stock|unit_price|\n",
            "+---------+------+-----+----------+\n",
            "| iPhone 6| Apple|    6|     549.0|\n",
            "|iPhone 6s| Apple|    5|     585.0|\n",
            "| iPhone 7| Apple|   11|     739.0|\n",
            "|    Pixel|Google|    8|     859.0|\n",
            "| Pixel XL|Google|    2|     959.0|\n",
            "+---------+------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.filter((phone_df.brand == 'Apple') | (phone_df.brand == 'Google')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GxDUtbu_7NY6"
      },
      "source": [
        "### Trier les lignes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xMv8c0ZB7NY6"
      },
      "source": [
        "Nous pouvons utiliser l'opération [`orderBy`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html) pour trier les données :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lZDf_gHB7NY6",
        "outputId": "a320edbf-631d-48b5-d398-f41328755101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+-----+----------+\n",
            "|        model|  brand|stock|unit_price|\n",
            "+-------------+-------+-----+----------+\n",
            "|    Galaxy A5|Samsung|    7|     297.0|\n",
            "|    Galaxy S6|Samsung|    5|     414.0|\n",
            "|    Galaxy S7|Samsung|   10|     539.0|\n",
            "|     iPhone 6|  Apple|    6|     549.0|\n",
            "|    iPhone 6s|  Apple|    5|     585.0|\n",
            "|     iPhone 7|  Apple|   11|     739.0|\n",
            "|Galaxy Note 7|Samsung|    0|     841.0|\n",
            "|        Pixel| Google|    8|     859.0|\n",
            "|     Pixel XL| Google|    2|     959.0|\n",
            "+-------------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.orderBy('unit_price').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "88OsBj-f7NY6"
      },
      "source": [
        "Dans la cellule suivante, nous utilisons une chaîne de méthodes DataFrame qui sont très similaires au langage de requête SQL utilisé pour certaines bases de données.\n",
        "    Notez que nous n'utilisons que les noms des colonnes. Notez également l'utilisation de guillemets doubles et simples dans la méthode [`where`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.where.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UZ2th8kV7NY9",
        "outputId": "66857142-a71c-498e-cc70-d629d41dd914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|    model|unit_price|\n",
            "+---------+----------+\n",
            "| iPhone 7|     739.0|\n",
            "| iPhone 6|     549.0|\n",
            "|iPhone 6s|     585.0|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "phone_df.select(\"model\", \"unit_price\").where(\"brand='Apple'\").orderBy('stock', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Yw1qxA5p7NY9"
      },
      "source": [
        "Une autre façon de faire la même chose que la cellule ci-dessus est d'utiliser `phone_df[« brand »]` dans la clause where. C'est plus long à taper mais intuitivement plus clair et plus facile à lire. Il n'y a pas d'ambiguïté pour l'analyseur Spark avec cette notation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cKbbtnMV7NY9"
      },
      "outputs": [],
      "source": [
        "phone_df.select(\"model\", \"unit_price\").where(phone_df[\"brand\"]==\"Apple\").orderBy('stock', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rx59FcD_7NY-"
      },
      "source": [
        "## Exercice 3\n",
        "Sélectionner tous les téléphones dont le prix unitaire est supérieur à 300 et dont il y a plus de deux en stock. Affichez les téléphones restants, classés par marque, puis par stock. Utilisez la syntaxe de spécification de colonne que vous préférez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "UdZ1K6X37NY-",
        "outputId": "df756a06-6e67-4f39-efe2-e2c07096b804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-13-45a4860a2a25>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-45a4860a2a25>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    <FILL IN>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# TODO: Remplacer <FILL IN> avec le code approprié\n",
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6lMPRser7NY-"
      },
      "source": [
        " ## Agrégation des données\n",
        "Une partie importante du traitement des données est la possibilité de combiner plusieurs enregistrements. Dans l'API DataFrame, il s'agit d'un processus en deux étapes :\n",
        "\n",
        "D'abord, vous regroupez les données en utilisant la méthode `groupBy`. `groupBy` peut opérer sur une ou plusieurs colonnes. Elle n'effectue pas le regroupement mais crée une référence à un objet `GroupedData` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "etM-jqUw7NY-",
        "outputId": "272706fa-bcd3-476c-ad5d-2a2f2881e50d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.group.GroupedData'>\n"
          ]
        }
      ],
      "source": [
        "grouped_df = phone_df.groupBy('brand')\n",
        "print(type(grouped_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hykJeqte7NY-"
      },
      "source": [
        "Une fois les données regroupées, nous pouvons leur appliquer l'une des fonctions d'agrégation standard. Elles sont répertoriées dans la documentation de l'API [GroupedData](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html) de l'API. Il s'agit de : `min`, `max`, `mean`, `sum` et `count`.\n",
        "\n",
        "Nous pouvons appliquer une agrégation à toutes les colonnes ou à un sous-ensemble de colonnes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "uoeFh0qe7NY-"
      },
      "outputs": [],
      "source": [
        "# Minimum for all columns\n",
        "min_df = grouped_df.min('unit_price')\n",
        "\n",
        "min_df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cHOpvYpY7NY-"
      },
      "source": [
        "Notez que `min(unit_price)` est le nom de la nouvelle colonne.\n",
        "\n",
        "Si vous souhaitez renommer une colonne, utilisez [`withColumnRenamed`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnRenamed.html). Cette méthode prend comme arguments l'ancien et le nouveau nom de la colonne.\n",
        "\n",
        "Notez également qu'on a transformé le DataFrame Spark en DataFrame Pandas avec `toPandas`. Si on voulait rester 'pure Spark', on pourrait faire appel à `show()`, par exemple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Fm5C9C8y7NY-"
      },
      "source": [
        "## Exercice 4\n",
        "\n",
        "Calculez le maximum du prix unitaire par marque et renommez la colonne résultante en `max`.\n",
        "\n",
        "Nous supposons que vous pouvez faire cela en une seule ligne. N'hésitez pas à adapter la cellule et à utiliser plus de lignes si vous le souhaitez.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4J7_SEC27NY-",
        "outputId": "a77765de-cddc-4e00-c049-02ee56557e74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+\n",
            "|  brand|max(unit_price)|\n",
            "+-------+---------------+\n",
            "| Google|          959.0|\n",
            "|  Apple|          739.0|\n",
            "|Samsung|          841.0|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Remplacer <FILL IN> avec le code approprié\n",
        "max_df = <FILL IN>\n",
        "max_df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "w277Tz0K7NY_"
      },
      "source": [
        "Dans certains cas, on souhaite effectuer des aggrégations différentes sur plusieurs colonnes. Dans ce cas, nous pouvons combiner les différentes agrégations par colonne en utilisant la méthode [`agg`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) sur une instance de GroupedData :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "DM1m3NJ-7NY_",
        "outputId": "61c5eeb2-123a-4484-82a9-79dc48b9f52b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------------+\n",
            "|  brand|sum(stock)|  avg(unit_price)|\n",
            "+-------+----------+-----------------+\n",
            "| Google|        10|            909.0|\n",
            "|  Apple|        22|624.3333333333334|\n",
            "|Samsung|        22|           522.75|\n",
            "+-------+----------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fait la somme de la colonne stock et la moyenne de la colonne unit_price\n",
        "sum_df = grouped_df.agg({'stock': 'sum', 'unit_price': 'mean'})\n",
        "\n",
        "sum_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Shjx8mIc7NY_"
      },
      "source": [
        "## SQL\n",
        "L'API SQL permet d'interroger les dataframes avec une syntaxe compatible SQL (utilisée dans la plupart des bases de données). Vous pouvez accéder à l'API SQL à partir de la session Spark en utilisant `spark.sql`. Voici comparaison entre une requête effectuée en utilisant l'API DataFrame de Spark et une requête avec l'API SQL :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GpNt7O3G7NY_",
        "outputId": "e2cbf7a5-b0cf-46d2-83c2-9116c874ecc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|    model|\n",
            "+---------+\n",
            "| iPhone 7|\n",
            "|    Pixel|\n",
            "|Galaxy S7|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DataFrame version\n",
        "res_df = phone_df.filter(phone_df['stock'] > 7).select('model')\n",
        "res_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_yrKxPIN7NY_"
      },
      "source": [
        "La version SQL de la requête exige que nous « enregistrions » le DataFrame en tant que table SQL :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ADW6uwsy7NY_",
        "outputId": "d34b13ca-3cc4-44b3-95ad-e04706e38544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|    model|\n",
            "+---------+\n",
            "| iPhone 7|\n",
            "|    Pixel|\n",
            "|Galaxy S7|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SQL version\n",
        "\n",
        "# Register the phone_df DataFrame within SQL as a table with name 'phones'\n",
        "phone_df.createOrReplaceTempView('phones')\n",
        "\n",
        "# Perform the SQL query on the 'phones' table\n",
        "res_df = spark.sql('SELECT model FROM phones WHERE stock > 7')\n",
        "res_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xhfgseai7NY_"
      },
      "source": [
        "## Jointure avec d'autres ensembles de données\n",
        "Il arrive souvent que vous souhaitiez combiner plusieurs ensembles de données en se basant sur une même colonne (clé commune). Dans cet exemple avec l'API Spark DataFrame, nous créons une table supplémentaire contenant des informations sur le fabricant du téléphone :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Kqy8060f7NY_",
        "outputId": "c4d2cf32-0a33-4bb1-a508-48c346caa579",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+-------------+-------------+\n",
            "|company_name| hq_country|founding_year|          ceo|\n",
            "+------------+-----------+-------------+-------------+\n",
            "|      Google|        USA|         1998|Sundar Pichai|\n",
            "|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|       Apple|        USA|         1976|     Tim Cook|\n",
            "+------------+-----------+-------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "companies = [\n",
        "    ('Google', 'USA', 1998, 'Sundar Pichai'),\n",
        "    ('Samsung', 'South Korea', 1938 ,'Oh-Hyun Kwon' ),\n",
        "    ('Apple', 'USA', 1976 ,'Tim Cook')\n",
        "]\n",
        "\n",
        "columns = ['company_name', 'hq_country', 'founding_year', 'ceo']\n",
        "\n",
        "company_df = spark.createDataFrame(companies, columns)\n",
        "company_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EwfT1dD07NZA"
      },
      "source": [
        "Pour joindre deux DataFrames, nous utilisons la méthode `join` sur l'un des DataFrames. Cette méthode prend deux arguments :  \n",
        "\n",
        "1.   l'autre DataFrame, et\n",
        "2.   une relation de jointure.\n",
        "\n",
        "Ici, nous joignons les deux DataFrames sur les colonnes brand/company_name :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1TDOiMvv7NZA",
        "outputId": "7a5411c3-7965-41ac-d6e4-b8656147f5ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "|        model|  brand|stock|unit_price|company_name| hq_country|founding_year|          ceo|\n",
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "|     iPhone 6|  Apple|    6|     549.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|    iPhone 6s|  Apple|    5|     585.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|     iPhone 7|  Apple|   11|     739.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|        Pixel| Google|    8|     859.0|      Google|        USA|         1998|Sundar Pichai|\n",
            "|     Pixel XL| Google|    2|     959.0|      Google|        USA|         1998|Sundar Pichai|\n",
            "|    Galaxy S7|Samsung|   10|     539.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|    Galaxy S6|Samsung|    5|     414.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|    Galaxy A5|Samsung|    7|     297.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|Galaxy Note 7|Samsung|    0|     841.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joined_df = phone_df.join(company_df, phone_df['brand'] == company_df['company_name'])\n",
        "joined_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, nous avons la même jointure en utilisant SQL. Notez qu'on doit également déclarer le dataframe `company_df` en tant que table (avec le nom `company`)."
      ],
      "metadata": {
        "id": "TTS8ZH5ZGKJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the phone_df DataFrame within SQL as a table with name 'phones'\n",
        "company_df.createOrReplaceTempView('company')\n",
        "\n",
        "# Perform the SQL query on the 'phones' table\n",
        "join_df = spark.sql('SELECT * FROM phones JOIN company ON phones.brand = company.company_name')\n",
        "join_df.show()"
      ],
      "metadata": {
        "id": "zbj_IdJQFaQk",
        "outputId": "ffdfa189-2f65-4c1d-8bbe-78205f174b4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "|        model|  brand|stock|unit_price|company_name| hq_country|founding_year|          ceo|\n",
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "|     iPhone 6|  Apple|    6|     549.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|    iPhone 6s|  Apple|    5|     585.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|     iPhone 7|  Apple|   11|     739.0|       Apple|        USA|         1976|     Tim Cook|\n",
            "|        Pixel| Google|    8|     859.0|      Google|        USA|         1998|Sundar Pichai|\n",
            "|     Pixel XL| Google|    2|     959.0|      Google|        USA|         1998|Sundar Pichai|\n",
            "|    Galaxy S7|Samsung|   10|     539.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|    Galaxy S6|Samsung|    5|     414.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|    Galaxy A5|Samsung|    7|     297.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "|Galaxy Note 7|Samsung|    0|     841.0|     Samsung|South Korea|         1938| Oh-Hyun Kwon|\n",
            "+-------------+-------+-----+----------+------------+-----------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZPtESzsZ7NZA"
      },
      "source": [
        "Here is an example of a more complicated query that combines multiple steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "xCCNb75Y7NZA"
      },
      "outputs": [],
      "source": [
        "# All the models from USA companies with more than 7 items in stock\n",
        "result = phone_df \\\n",
        "    .join(company_df, phone_df['brand'] == company_df['company_name']) \\\n",
        "    .filter(company_df['hq_country'] == 'USA') \\\n",
        "    .filter(phone_df['stock'] > 7) \\\n",
        "    .select('model')\n",
        "\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kDc_uA6E7NZB"
      },
      "source": [
        "## Assignment 5\n",
        "\n",
        "The problem below was taken from Coursera's MOOC [Big Data Analysis with Scala and Spark](https://www.coursera.org/learn/scala-spark-big-data) by the École Polytechnique Fédérale de Lausanne. We adapted the problem for PySpark.\n",
        "\n",
        "Let's assume we have a dataset with posts from a discussion forum. The entries of the dataset consist of an authorID, the name of a subforum, the number of likes and a date. The data frame is constructed in the following cell.\n",
        "\n",
        "**We would like to know how many likes each author posted on each subforum. The table should show per subforum how many likes each author has, the highest number of likes first.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GFFUYBvZ7NZB"
      },
      "outputs": [],
      "source": [
        "from  pyspark.sql import Row\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "\n",
        "posts = [{'authorID' : 4, 'subforum': 'java', 'likes': 5, 'date' : 'sept 5'},\n",
        "         {'authorID' : 1, 'subforum': 'python', 'likes': 3, 'date' : 'sept 4'},\n",
        "        {'authorID' : 2, 'subforum': 'python', 'likes': 35, 'date' : 'sept 3'},\n",
        "        {'authorID' : 3, 'subforum': 'java', 'likes': 1, 'date' : 'sept 5'},\n",
        "        {'authorID' : 4, 'subforum': 'java', 'likes': 14, 'date' : 'sept 5'},\n",
        "        {'authorID' : 3, 'subforum': 'python', 'likes': 12, 'date' : 'sept 3'},\n",
        "        {'authorID' : 3, 'subforum': 'java', 'likes': 14, 'date' : 'sept 5'},\n",
        "        {'authorID' : 3, 'subforum': 'java', 'likes': 10, 'date' : 'sept 5'},\n",
        "        {'authorID' : 2, 'subforum': 'python', 'likes': 21, 'date' : 'sept 5'}]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(posts)\n",
        "df_posts = spark.createDataFrame(rdd.map(lambda x : Row(**x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EDJYl0RN7NZB"
      },
      "source": [
        "Please use a [groupBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html), the [sum aggregation function](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.sum.html) and an [orderBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html) to come up with the desired dataFrame. Note that you want to order in descending order.\n",
        "Also note, that you can use [`groupBy`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html) and `orderBy` on more than one column.\n",
        "\n",
        "If you get confused, break the problem into steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b_RdWH1v7NZB"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "VDRy4or27NZB"
      },
      "source": [
        "## Conversion to/from RDD\n",
        "\n",
        "Sometimes you want to do data manipulations which would be very easy with RDD operations, but complicated with the DataFrame API. Fortunately you can convert between DataFrames and RDDs of type 'Row'. Going from DataFrame to RDD is quite simple. Going back from RDD to DataFrame is more difficult because you need to re-apply the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "LjiQp4pt7NZB"
      },
      "outputs": [],
      "source": [
        "phone_rdd = phone_df.rdd\n",
        "plural_rdd = phone_rdd.map(lambda r: r.brand + 's')\n",
        "plural_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ENWsh5vh7NZB"
      },
      "source": [
        "# Reading structured files/sources\n",
        "One of the advantages of DataFrames is the ability to read already structured data and automatically import the structure in Spark. Spark contains readers for a number of formats such as csv, json, parquet, orc, text and jdbc. There are also third-party readers/connectors for databases such as MongoDB and Cassandra.\n",
        "\n",
        "Here we read the json-formatted tweets that we also used in the last notebook. As you can see the complicated JSON schema is inferred."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lsteffenel/pyspark-binder/raw/master/tweets.json"
      ],
      "metadata": {
        "id": "T6xagZnR7Zew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b21zGGur7NZB"
      },
      "outputs": [],
      "source": [
        "tweet_df = spark.read.format(\"json\").load('tweets.json')\n",
        "tweet_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1RyhiIn07NZC"
      },
      "source": [
        "This structure is squeezed into a table. When we convert to Pandas we can see what the first tweet looks like in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "60HOv47A7NZC"
      },
      "outputs": [],
      "source": [
        "tweet_df.toPandas().head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3AwQPl3o7NZC"
      },
      "source": [
        "## Assignment 6\n",
        "Select the name and screen_name of the user, the text field and the lang field.\n",
        "\n",
        "**Hint**: nested fields can be selected using the dot notation, i.e. `df.select('<parent>.<child>')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Y3Ih9XMA7NZC"
      },
      "outputs": [],
      "source": [
        "name_df = tweet_df.<FILL IN>\n",
        "name_df.toPandas().head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "AC2Hcizh7NZC"
      },
      "source": [
        "## Assignment 7\n",
        "Count the number of tweets per user, and display the top 10 most-tweeting users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lYy6YID07NZC"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ySHfiyNt7NZC"
      },
      "source": [
        "## Word count in DataFrames\n",
        "\n",
        "It is also possible to use DataFrames for less-structured data such as text. Here we show how you could do word count with DataFrames.\n",
        "\n",
        "The following chained query contains a number of methods you haven't seen before, and we'll go through it line by line."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lsteffenel/pyspark-binder/raw/master/shakespeare.txt"
      ],
      "metadata": {
        "id": "ICH9zZ5Y7gct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IOD9iZSC7NZC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode, split\n",
        "\n",
        "spark \\\n",
        "    .read.text('shakespeare.txt') \\\n",
        "    .select(explode(split(\"value\", \"\\W+\")).alias(\"word\")) \\\n",
        "    .groupBy(\"word\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"count\", ascending=0).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8eCyKiYV7NZC"
      },
      "source": [
        "To see what happens here, we break it down into steps. First we read in the data file and inspect the DataFrame. It contains one column, called `value` by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Pq0nJyfO7NZC"
      },
      "outputs": [],
      "source": [
        "swan_df = spark.read.text('shakespeare.txt')\n",
        "swan_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2raJUmGl7NZC"
      },
      "source": [
        "The column name `value` explains why it is mentioned inside the `split` function. Let's call the `select` method but omit `explode` and see what happens. Notice, that with `alias` we rename the column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-nJg0efo7NZD"
      },
      "outputs": [],
      "source": [
        "split_df = swan_df.select(split(\"value\", \"\\W+\").alias(\"word\"))\n",
        "split_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GtEkJUbx7NZD"
      },
      "source": [
        "Looking at the schema, we can see that `word` is actually an array of strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "wrWYp9tX7NZD"
      },
      "outputs": [],
      "source": [
        "split_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7txqDpMd7NZD"
      },
      "source": [
        "Instead, we would like to have a row for each word, which is where [`explode`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) comes in. It has a similar meaning as `flatMap` in Spark RDDs. It gets rid of lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "O0mldJtn7NZD"
      },
      "outputs": [],
      "source": [
        "swan_df.select(explode(split(\"value\", \"\\W+\")).alias(\"word\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vWw16ldo7NZD"
      },
      "source": [
        "### User-defined functions\n",
        "\n",
        "In the previous example we used the built-in split function. It is also possible to define and use a custom user-defined function, or UDF. We'll show an example for the phone stock DataFrame first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "00qiTIbn7NZD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "exp_udf = udf(lambda price: \"Expensive\" if price >= 500 else \"Inexpensive\", StringType())\n",
        "\n",
        "phone_df.withColumn(\"cost\", exp_udf(phone_df['unit_price'])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jnrZSQ-M7NZD"
      },
      "source": [
        "In this manner, we can apply specialized function, like tokenizers, on DataFrames. However, we first must register them as UDFs and cannot simply define them inline with lambda functions like we can with RDDs.\n",
        "\n",
        "Below we define a very simple tokenizer, just as an example. It uses Python's string `split`, and also lowers the case of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kUERqQvq7NZD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "def my_tokenize(s):\n",
        "    s = s.lower()\n",
        "    words = s.split()\n",
        "    return words\n",
        "\n",
        "returnType = ArrayType(StringType())\n",
        "\n",
        "tokenize_udf = udf(my_tokenize, returnType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "dolmvHbU7NZD"
      },
      "source": [
        "## Assignment 8\n",
        "Use the `my_tokenize` function from the last cell to count words on the Shakespeare DataFrame `swan_df` instead of usng the `split` function. Display the top 10 most occurring words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ZbZPXIFg7NZE"
      },
      "outputs": [],
      "source": [
        "<FILL IN>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QJbkhTgO7NZE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}