{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3130b70e-ba56-430a-99db-15282695ca2c",
      "metadata": {
        "id": "3130b70e-ba56-430a-99db-15282695ca2c"
      },
      "source": [
        "# Travaillant avec Spark Structured Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5c629cc-e470-4e44-8018-2c8268a15ea7",
      "metadata": {
        "id": "c5c629cc-e470-4e44-8018-2c8268a15ea7"
      },
      "source": [
        "Tout comme Spark a migré de l'API \"bas niveau\" des RDD vers l'API déclarative DataFrame, le support au streaming a aussi deux bibliothèques différentes :\n",
        "* Spark Streaming - ancienne bibliothèque, axée sur les RDD\n",
        "* Spark Structured Streaming - nouvelle version, intégrant les DataFrames via spark.sql\n",
        "\n",
        "La différence va au delà des RDD et des DataFrames. Par exemple, `Structured Streaming` adopte un traitement basé sur le **event time** plutôt que par ordre d'arrivée (**processing time**). Cela permet de traiter les événéments retardés par la transmission, mais demande plus d'organisation (suivi de l'état des événéments, fenêtres de timeout).\n",
        "\n",
        "Autre différence, l'ancienne version ne suportait que le processement par **micro batches**, alors que Structured Streaming permet aussi le **continuous processing**. Cette dernière option permet une plus pétite latence de traitement car on n'attend plus qu'un certain nombre d'entrées soit arrivée.\n",
        "\n",
        "Dans les paragraphes suivants on va regarder quelques exemples d'utilisation de Structured Streaming. Nous allons utiliser des données issues d'un suivi des activités d'une personne (https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98fa8f4f-281c-4e6e-a307-26a589c23888",
      "metadata": {
        "id": "98fa8f4f-281c-4e6e-a307-26a589c23888"
      },
      "source": [
        "## Démarrage d'une session Spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "WjKghEspBwzS"
      },
      "id": "WjKghEspBwzS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a8c3c9-c34f-41f1-aac6-938f0b7162a0",
      "metadata": {
        "id": "a1a8c3c9-c34f-41f1-aac6-938f0b7162a0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991b792c-6d7f-4041-bedc-15d64d56f49c",
      "metadata": {
        "id": "991b792c-6d7f-4041-bedc-15d64d56f49c"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"readfromjson\").master(\"local[2]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a774bdfe-c1da-4f89-9504-a728c69e5d93",
      "metadata": {
        "id": "a774bdfe-c1da-4f89-9504-a728c69e5d93"
      },
      "source": [
        "Dans cet exercice nous allons simuler un flux de données en lisant plusieurs fichiers au format json.\n",
        "Comme les DataFrames ont besoin du schéma des données, l'étape suivante prend un peu de temps pour analyser la structure de tous les fichiers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://urca.lsteffenel.fr/activity-data.zip\n",
        "!unzip activity-data.zip"
      ],
      "metadata": {
        "id": "3wMPy_E2Bi1k"
      },
      "id": "3wMPy_E2Bi1k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e578e243-0f88-4259-8092-21c7f9d24ed1",
      "metadata": {
        "id": "e578e243-0f88-4259-8092-21c7f9d24ed1"
      },
      "outputs": [],
      "source": [
        "static = spark.read.json(\"activity-data/\")\n",
        "dataSchema = static.schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd3a20f-5718-4bda-a59f-c3a51dea8418",
      "metadata": {
        "id": "6fd3a20f-5718-4bda-a59f-c3a51dea8418"
      },
      "outputs": [],
      "source": [
        "static.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10eb8d0a-b699-47f5-827a-7b68c737f71f",
      "metadata": {
        "id": "10eb8d0a-b699-47f5-827a-7b68c737f71f"
      },
      "source": [
        "Nous allons maintenant définir le flux streaming. On renseigne le schéma et la source (`.json()`), puis on limite la lecture à un fichier à la fois pour avoir plus de temps de traitement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f539d0e1-3d90-47f5-8e91-07aad70f4b16",
      "metadata": {
        "id": "f539d0e1-3d90-47f5-8e91-07aad70f4b16"
      },
      "outputs": [],
      "source": [
        "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
        "  .json(\"activity-data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f798a8-cf76-4d82-96d2-41038a13d110",
      "metadata": {
        "id": "03f798a8-cf76-4d82-96d2-41038a13d110"
      },
      "source": [
        "Ensuite, nous définissions une transformation qui récupère les données, les regroupe par la colonne `gt` et met à jour le nombre d'éléments.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "329cae63-b62e-48f1-bda2-0f3a5ddd55fe",
      "metadata": {
        "id": "329cae63-b62e-48f1-bda2-0f3a5ddd55fe"
      },
      "outputs": [],
      "source": [
        "activityCounts = streaming.groupBy(\"gt\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86700751-83d1-4645-aacd-74f7e51b56a6",
      "metadata": {
        "id": "86700751-83d1-4645-aacd-74f7e51b56a6"
      },
      "source": [
        "Pour le moment ces actions n'ont pas été démarrées. La ligne suivante démarre le traitement du stream.\n",
        "\n",
        "Les données sont stockées en mémoire dans un tableau appelé **activity_counts**.\n",
        "\n",
        "Le mode de traitement est **complet**, ce que veut dire que l'ensemble du tableau est mis à jour à chaque événément."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d8e200-eabf-42f4-9811-54822f3b6840",
      "metadata": {
        "id": "e3d8e200-eabf-42f4-9811-54822f3b6840"
      },
      "outputs": [],
      "source": [
        "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
        "  .format(\"memory\").outputMode(\"complete\")\\\n",
        "  .start()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30bccc9-fa0b-4f2b-b835-41af11a80868",
      "metadata": {
        "id": "f30bccc9-fa0b-4f2b-b835-41af11a80868"
      },
      "source": [
        "Pour finir, nous allons essayer de visualiser ces mises à jour.\n",
        "\n",
        "La boucle suivante utilise la syntaxe `spark.sql`pour faire une requête sur le tableau *activity_counts*. En faisant des pauses d'une seconde entre chaque mise à jour, on peut visualiser l'évolution du remplissage du tableau."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b058f277-7dfe-4881-bde4-248ee8c38c9f",
      "metadata": {
        "id": "b058f277-7dfe-4881-bde4-248ee8c38c9f"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "for x in range(20):\n",
        "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
        "    sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a942fe4-759f-402e-ae14-4b6d3698e5a6",
      "metadata": {
        "id": "5a942fe4-759f-402e-ae14-4b6d3698e5a6"
      },
      "source": [
        "Le streaming est toujours actif, comme le prouve la ligne suivante.\n",
        "\n",
        "Dans certains cas (lors d'une longue exécution), le *driver* peut être arrêté lorsque le flux de données est interrompu pendant trop de temps. Afin d'éviter ça, on peut demander explicitement au driver d'attendre la fin (un Ctrl-C, par exemple) avec la méthode `.awaitTermination()`. Dans notre cas, on va se contenter de faire un `.stop()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d57d0af6-175c-4d7a-859b-6ea4687db71c",
      "metadata": {
        "id": "d57d0af6-175c-4d7a-859b-6ea4687db71c"
      },
      "outputs": [],
      "source": [
        "spark.streams.active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc8698f7-9138-4008-8f19-baa1f8413c87",
      "metadata": {
        "id": "fc8698f7-9138-4008-8f19-baa1f8413c87"
      },
      "outputs": [],
      "source": [
        "activityQuery.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3530983f-bd29-4766-9001-2844deb6c112",
      "metadata": {
        "id": "3530983f-bd29-4766-9001-2844deb6c112"
      },
      "source": [
        "## Travaillant avec les timestamps\n",
        "\n",
        "Le traitement des données au fur et à mesure de leur arrivée peut avoir des mauvaises conséquences si des messages sont perdus, retardés ou arrivent en doublon. Pour cela, Structured Streaming permet de manipuler les événéments et de contrôler la fenêtre d'utilité de ces données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d73865-95c4-4e1b-81eb-00e591c1d3b2",
      "metadata": {
        "id": "09d73865-95c4-4e1b-81eb-00e591c1d3b2"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
        "static = spark.read.json(\"activity-data\")\n",
        "streaming = spark\\\n",
        "  .readStream\\\n",
        "  .schema(static.schema)\\\n",
        "  .option(\"maxFilesPerTrigger\", 10)\\\n",
        "  .json(\"activity-data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a52e25-da72-4351-bafd-d696d350a3be",
      "metadata": {
        "id": "37a52e25-da72-4351-bafd-d696d350a3be"
      },
      "outputs": [],
      "source": [
        "streaming.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c67ef7e1-5cb5-479c-8dab-7e42598da82e",
      "metadata": {
        "id": "c67ef7e1-5cb5-479c-8dab-7e42598da82e"
      },
      "source": [
        "La colonne `Creation_Time` est représentée en tant que des nanosecondes (*unixtime*). Nous allons d'abord les transformer en quelque chose plus facile à travailler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbce5eb-4ac5-4e31-988b-811ad78325e3",
      "metadata": {
        "id": "bbbce5eb-4ac5-4e31-988b-811ad78325e3"
      },
      "outputs": [],
      "source": [
        "withEventTime = streaming.selectExpr(\n",
        "  \"*\",\n",
        "  \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c587a817-dd5e-4e9f-9540-3b32d716cc7c",
      "metadata": {
        "id": "c587a817-dd5e-4e9f-9540-3b32d716cc7c"
      },
      "source": [
        "### Traitement par blocs\n",
        "\n",
        "La façon la plus simple de traiter les événéments entrants est de définir une fenêtre de traitement (un peu comme un batch). Dans le code suivant, les événéments sont regroupés par tranches de 10 minutes, sans superposition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c9c50e1-0fa1-4024-b574-27cad601dff3",
      "metadata": {
        "id": "7c9c50e1-0fa1-4024-b574-27cad601dff3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import window, col\n",
        "event_bloc = withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\")).count()\\\n",
        "  .writeStream\\\n",
        "  .queryName(\"pyevents_per_window\")\\\n",
        "  .format(\"memory\")\\\n",
        "  .outputMode(\"complete\")\\\n",
        "  .start()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4908884e-60bf-4f13-aeb6-aff97ac66381",
      "metadata": {
        "id": "4908884e-60bf-4f13-aeb6-aff97ac66381"
      },
      "source": [
        "Attendez quelques instants. À la fin du traitement, on peut visualiser les données regroupées par blocs de 10 minutes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5827a0f6-7de1-44a5-ac40-476e71ce9941",
      "metadata": {
        "id": "5827a0f6-7de1-44a5-ac40-476e71ce9941"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM pyevents_per_window\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b695ea4-2ee2-4300-b9b6-a2b09a4f8a33",
      "metadata": {
        "id": "0b695ea4-2ee2-4300-b9b6-a2b09a4f8a33"
      },
      "outputs": [],
      "source": [
        "event_bloc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d88f91f-2d70-4540-8a6a-7ff80037d0d3",
      "metadata": {
        "id": "1d88f91f-2d70-4540-8a6a-7ff80037d0d3"
      },
      "source": [
        "### Fenêtre glissante\n",
        "\n",
        "Une autre façon de travailler est d'utiliser une fenêtre glissante pour mettre à jour le tableau. Dans le cas suivant, on déclenche une mise à jour à chaque 5 minutes, en utilisant les données reçues dans les 10 dernières minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482d0591-9fd2-4902-953b-0ac8aec2a7b6",
      "metadata": {
        "id": "482d0591-9fd2-4902-953b-0ac8aec2a7b6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import window, col\n",
        "event_window = withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
        "  .count()\\\n",
        "  .writeStream\\\n",
        "  .queryName(\"pyevents_per_window\")\\\n",
        "  .format(\"memory\")\\\n",
        "  .outputMode(\"complete\")\\\n",
        "  .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39d17c6-b8c9-435b-8a42-8f71a7b15691",
      "metadata": {
        "id": "d39d17c6-b8c9-435b-8a42-8f71a7b15691"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM pyevents_per_window\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "787b7528-4f94-43d9-8846-34e0c7cf8ceb",
      "metadata": {
        "id": "787b7528-4f94-43d9-8846-34e0c7cf8ceb"
      },
      "source": [
        "Répétez la ligne précédente quelques fois afin de voir l'avancée du traitement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a37b66-e9c9-4e39-924f-d1f73bc2e648",
      "metadata": {
        "id": "c6a37b66-e9c9-4e39-924f-d1f73bc2e648"
      },
      "outputs": [],
      "source": [
        "event_window.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e890ea33-1e2d-45db-8ed2-85c38d8cad79",
      "metadata": {
        "tags": [],
        "id": "e890ea33-1e2d-45db-8ed2-85c38d8cad79"
      },
      "source": [
        "### Watermark\n",
        "\n",
        "Dans certains cas, les données peuvent arriver trop tard. Afin de limiter l'impact sur le traitement, on peut limiter le temps maximum d'attente (*watermarking*).\n",
        "\n",
        "L'exemple suivant donne un aperçu de cette procédure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7b8008-7bb1-4b77-820f-ab97a528a4a4",
      "metadata": {
        "id": "8b7b8008-7bb1-4b77-820f-ab97a528a4a4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import window, col\n",
        "event_watermark = withEventTime\\\n",
        "  .withWatermark(\"event_time\", \"30 minutes\")\\\n",
        "  .groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
        "  .count()\\\n",
        "  .writeStream\\\n",
        "  .queryName(\"pyevents_per_window\")\\\n",
        "  .format(\"memory\")\\\n",
        "  .outputMode(\"complete\")\\\n",
        "  .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce70b28-c1c4-46cf-b38c-109bf5365717",
      "metadata": {
        "id": "4ce70b28-c1c4-46cf-b38c-109bf5365717"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM pyevents_per_window\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82a7757-693e-4e80-b89f-3d87677be38b",
      "metadata": {
        "tags": [],
        "id": "f82a7757-693e-4e80-b89f-3d87677be38b"
      },
      "source": [
        "Répétez la ligne précédente quelques fois afin de voir l'avancée du traitement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e619c523-21ff-4f99-9d92-bb67ebd5f412",
      "metadata": {
        "id": "e619c523-21ff-4f99-9d92-bb67ebd5f412"
      },
      "outputs": [],
      "source": [
        "event_watermark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62fed45-3516-475a-a94b-8989d04e2bed",
      "metadata": {
        "id": "f62fed45-3516-475a-a94b-8989d04e2bed"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}